{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NINDS EOPD WGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### July 25, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notebook for processing the NINDS EOPD WGS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### global varibles and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global notebook variables for both python and bash majic (by stdin arguments)\n",
    "WRKDIR = '/labseq/projects/ninds_eopd'\n",
    "PRJ_BUCKET = 'gs://nihnialng-pd-wgs'\n",
    "PROJECT_ID = 'pd-genome'\n",
    "MYUSER = 'gibbsr'\n",
    "AUTOSOMES=[str(x) for x in list(range(1,23))]\n",
    "SEXOMES=['X','Y']\n",
    "CHROMOSOMES=AUTOSOMES + SEXOMES\n",
    "COHORT='ninds_eopd'\n",
    "COHORTBUILD='{}.july2019'.format(COHORT)\n",
    "COHORT_BUCKET='{}/{}'.format(PRJ_BUCKET,COHORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### see what fastqs are present in the stagin bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592\n",
      "592\n",
      "gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.00/A3-35863_S21_L001_R1_001.fastq.gz\n",
      "gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.00/A3-35863_S21_L001_R2_001.fastq.gz\n",
      "gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.00/A3-35863_S21_L002_R1_001.fastq.gz\n",
      "gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.00/A3-35863_S21_L002_R2_001.fastq.gz\n",
      "gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.00/A3-35863_S21_L003_R1_001.fastq.gz\n",
      "gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.00/A3-35863_S21_L003_R2_001.fastq.gz\n",
      "gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.00/A3-35863_S21_L004_R1_001.fastq.gz\n",
      "gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.00/A3-35863_S21_L004_R2_001.fastq.gz\n",
      "gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.00/A3-35864_S22_L001_R1_001.fastq.gz\n",
      "gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.00/A3-35864_S22_L001_R2_001.fastq.gz\n",
      "gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.01/A3-35940_S23_L001_R1_001.fastq.gz\n",
      "gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.01/A3-35940_S23_L001_R2_001.fastq.gz\n",
      "gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.01/A3-35940_S23_L002_R1_001.fastq.gz\n",
      "gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.01/A3-35940_S23_L002_R2_001.fastq.gz\n",
      "gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.01/A3-35940_S23_L003_R1_001.fastq.gz\n",
      "gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.01/A3-35940_S23_L003_R2_001.fastq.gz\n",
      "gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.01/A3-35940_S23_L004_R1_001.fastq.gz\n",
      "gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.01/A3-35940_S23_L004_R2_001.fastq.gz\n",
      "gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.01/A3-35941_S24_L001_R1_001.fastq.gz\n",
      "gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.01/A3-35941_S24_L001_R2_001.fastq.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#staging bucket that USUHS uploaded fastqs to\n",
    "gsutil ls gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.00 | wc -l\n",
    "gsutil ls gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.01 | wc -l\n",
    "    \n",
    "gsutil ls gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.00 | head\n",
    "gsutil ls gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.01 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#run these commands at terminal:\n",
      "\n",
      "#WE'VE ALREADY RUN THIS SO DO NOT NEED TO COPY AGAIN\n",
      "\n",
      "gsutil -mq cp gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.0*/*.fastq.gz gs://nihnialng-pd-wgs/ninds_eopd/fastqs/\n"
     ]
    }
   ],
   "source": [
    "dest_bucket_path = '{}/{}/fastqs'.format(PRJ_BUCKET,COHORT)\n",
    "gcs_fastq_mv_cmd = 'gsutil -mq cp gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.0*/*.fastq.gz \\\n",
    "{}/'.format(dest_bucket_path)\n",
    "\n",
    "print('#run these commands at terminal:\\n')\n",
    "print('#WE\\'VE ALREADY RUN THIS SO DO NOT NEED TO COPY AGAIN\\n')\n",
    "print(gcs_fastq_mv_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592\n",
      "592\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$dest_bucket_path\"\n",
    "#check all the fastqs have been moved should be 592 pairs\n",
    "DEST_BUCKET=${1}\n",
    "\n",
    "gsutil ls ${DEST_BUCKET}/*_R1_001.fastq.gz | wc -l\n",
    "gsutil ls ${DEST_BUCKET}/*_R2_001.fastq.gz | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get fastqs listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1184\n",
      "A3-35863\tS21\tL001\tR1\t001\n",
      "A3-35863\tS21\tL001\tR2\t001\n",
      "A3-35863\tS21\tL002\tR1\t001\n",
      "A3-35863\tS21\tL002\tR2\t001\n",
      "A3-35863\tS21\tL003\tR1\t001\n",
      "A3-35863\tS21\tL003\tR2\t001\n",
      "A3-35863\tS21\tL004\tR1\t001\n",
      "A3-35863\tS21\tL004\tR2\t001\n",
      "A3-35864\tS22\tL001\tR1\t001\n",
      "A3-35864\tS22\tL001\tR2\t001\n",
      "A3-36014\tS23\tL004\tR1\t001\n",
      "A3-36014\tS23\tL004\tR2\t001\n",
      "A3-36015\tS24\tL001\tR1\t001\n",
      "A3-36015\tS24\tL001\tR2\t001\n",
      "A3-36015\tS24\tL002\tR1\t001\n",
      "A3-36015\tS24\tL002\tR2\t001\n",
      "A3-36015\tS24\tL003\tR1\t001\n",
      "A3-36015\tS24\tL003\tR2\t001\n",
      "A3-36015\tS24\tL004\tR1\t001\n",
      "A3-36015\tS24\tL004\tR2\t001\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$WRKDIR\" \"$dest_bucket_path\" \"$COHORT\"\n",
    "#get fastq listing and tokenize\n",
    "WRKDIR=${1}\n",
    "DEST_BUCKET=${2}\n",
    "COHORT=${3}\n",
    "\n",
    "FASTQ_LISTING=${WRKDIR}/${COHORT}.fastq.listing.txt\n",
    "\n",
    "gsutil ls ${DEST_BUCKET}/*.fastq.gz > ${FASTQ_LISTING}\n",
    "\n",
    "sed -i s\"/gs:\\/\\/nihnialng-pd-wgs\\/${COHORT}\\/fastqs\\///\"g ${FASTQ_LISTING}\n",
    "sed -i s\"/\\.fastq\\.gz//\"g ${FASTQ_LISTING}\n",
    "sed -i s\"/_/\\\\t/\"g ${FASTQ_LISTING}\n",
    "\n",
    "less ${FASTQ_LISTING} | wc -l\n",
    "head ${FASTQ_LISTING}\n",
    "tail ${FASTQ_LISTING}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load fastq listing and USUHS sample info and ID mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148, 97)\n",
      "(1184, 5)\n",
      "(148, 3)\n"
     ]
    }
   ],
   "source": [
    "#ok now see what we have\n",
    "#load the USUHS quality reports\n",
    "usuhs_qa_file = WRKDIR + '/pA3.QA.june.26.2019.xlsx'\n",
    "usuhs_qa = pd.read_excel(usuhs_qa_file)\n",
    "print(usuhs_qa.shape)\n",
    "\n",
    "#load the fastq info\n",
    "fastqs_file = '{}/{}.fastq.listing.txt'.format(WRKDIR,COHORT)\n",
    "fastqs_df = pd.read_csv(fastqs_file,header=None,sep='\\t')\n",
    "fastqs_df.columns = ['usuhsID','S','LANE','READ','NUM']\n",
    "print(fastqs_df.shape)\n",
    "\n",
    "#load usuhs key map\n",
    "usuhs_key_file = WRKDIR + '/pA3.hernandez.export1.keyTable.june.27.2019.csv'\n",
    "usuhs_keys_df = pd.read_csv(usuhs_key_file,sep=',')\n",
    "print(usuhs_keys_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SampleID</th>\n",
       "      <th>Flowcell</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3-35863</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35863-PennEOPD001-A01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3-35864</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35864-PennEOPD002-B01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3-35865</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35865-PennEOPD004-C01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3-35866</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35866-PennEOPD006-D01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3-35893</td>\n",
       "      <td>190419_N04_0056_BHK37HDSXX</td>\n",
       "      <td>A3-35893-PennEOPD046-G04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SampleID                    Flowcell               Description\n",
       "0  A3-35863  190419_N03_0073_AHJF53DSXX  A3-35863-PennEOPD001-A01\n",
       "1  A3-35864  190419_N03_0073_AHJF53DSXX  A3-35864-PennEOPD002-B01\n",
       "2  A3-35865  190419_N03_0073_AHJF53DSXX  A3-35865-PennEOPD004-C01\n",
       "3  A3-35866  190419_N03_0073_AHJF53DSXX  A3-35866-PennEOPD006-D01\n",
       "4  A3-35893  190419_N04_0056_BHK37HDSXX  A3-35893-PennEOPD046-G04"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(usuhs_keys_df.shape)\n",
    "usuhs_keys_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unfortunately of 'real' sampleID is packed in the middle of the 'Description' column values, extract it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "usuhs_keys_df['grbg1'],usuhs_keys_df['grbg2'],usuhs_keys_df['oriID'],usuhs_keys_df['grbg3'] = \\\n",
    "usuhs_keys_df['Description'].str.split('-').str\n",
    "usuhs_keys_df.drop(['grbg1','grbg2','grbg3'],axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SampleID</th>\n",
       "      <th>Flowcell</th>\n",
       "      <th>Description</th>\n",
       "      <th>oriID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3-35863</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35863-PennEOPD001-A01</td>\n",
       "      <td>PennEOPD001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3-35864</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35864-PennEOPD002-B01</td>\n",
       "      <td>PennEOPD002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3-35865</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35865-PennEOPD004-C01</td>\n",
       "      <td>PennEOPD004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3-35866</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35866-PennEOPD006-D01</td>\n",
       "      <td>PennEOPD006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3-35893</td>\n",
       "      <td>190419_N04_0056_BHK37HDSXX</td>\n",
       "      <td>A3-35893-PennEOPD046-G04</td>\n",
       "      <td>PennEOPD046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SampleID                    Flowcell               Description        oriID\n",
       "0  A3-35863  190419_N03_0073_AHJF53DSXX  A3-35863-PennEOPD001-A01  PennEOPD001\n",
       "1  A3-35864  190419_N03_0073_AHJF53DSXX  A3-35864-PennEOPD002-B01  PennEOPD002\n",
       "2  A3-35865  190419_N03_0073_AHJF53DSXX  A3-35865-PennEOPD004-C01  PennEOPD004\n",
       "3  A3-35866  190419_N03_0073_AHJF53DSXX  A3-35866-PennEOPD006-D01  PennEOPD006\n",
       "4  A3-35893  190419_N04_0056_BHK37HDSXX  A3-35893-PennEOPD046-G04  PennEOPD046"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usuhs_keys_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x.X</th>\n",
       "      <th>x.runFolder</th>\n",
       "      <th>x.Sample_ID</th>\n",
       "      <th>x.LaneNumbers</th>\n",
       "      <th>x.fastqCount</th>\n",
       "      <th>x.sampleSheet</th>\n",
       "      <th>x.seqInSS</th>\n",
       "      <th>x.seqInSSpre</th>\n",
       "      <th>x.seqOutSS</th>\n",
       "      <th>x.ssDiffFlag</th>\n",
       "      <th>...</th>\n",
       "      <th>x.meanCoverageP</th>\n",
       "      <th>x.PCT_20X</th>\n",
       "      <th>x.PCT_30X</th>\n",
       "      <th>x.NA.</th>\n",
       "      <th>x.NA..1</th>\n",
       "      <th>x.NA..2</th>\n",
       "      <th>x.NA..3</th>\n",
       "      <th>x.NA..4</th>\n",
       "      <th>x.NA..5</th>\n",
       "      <th>x.valTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9341</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35863</td>\n",
       "      <td>1~2~3~4</td>\n",
       "      <td>8</td>\n",
       "      <td>/data/seq2/seqIn//NovaSeq/190419_N03_0073_AHJF...</td>\n",
       "      <td>2019-04-24_16:00:36.7968139030_-0400</td>\n",
       "      <td>2019-04-22_11:23:26.0000000000_-0400</td>\n",
       "      <td>2019-04-24_16:00:41.3045690060_-0400</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>45.614280</td>\n",
       "      <td>0.963868</td>\n",
       "      <td>0.943429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-06-12 12:06:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9342</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35864</td>\n",
       "      <td>1~2~3~4</td>\n",
       "      <td>8</td>\n",
       "      <td>/data/seq2/seqIn//NovaSeq/190419_N03_0073_AHJF...</td>\n",
       "      <td>2019-04-24_16:00:36.7968139030_-0400</td>\n",
       "      <td>2019-04-22_11:23:26.0000000000_-0400</td>\n",
       "      <td>2019-04-24_16:00:41.3045690060_-0400</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>43.295565</td>\n",
       "      <td>0.962888</td>\n",
       "      <td>0.934077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-06-12 12:06:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9343</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35865</td>\n",
       "      <td>1~2~3~4</td>\n",
       "      <td>8</td>\n",
       "      <td>/data/seq2/seqIn//NovaSeq/190419_N03_0073_AHJF...</td>\n",
       "      <td>2019-04-24_16:00:36.7968139030_-0400</td>\n",
       "      <td>2019-04-22_11:23:26.0000000000_-0400</td>\n",
       "      <td>2019-04-24_16:00:41.3045690060_-0400</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.469096</td>\n",
       "      <td>0.960691</td>\n",
       "      <td>0.913079</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-06-12 12:06:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9344</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35866</td>\n",
       "      <td>1~2~3~4</td>\n",
       "      <td>8</td>\n",
       "      <td>/data/seq2/seqIn//NovaSeq/190419_N03_0073_AHJF...</td>\n",
       "      <td>2019-04-24_16:00:36.7968139030_-0400</td>\n",
       "      <td>2019-04-22_11:23:26.0000000000_-0400</td>\n",
       "      <td>2019-04-24_16:00:41.3045690060_-0400</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>42.829397</td>\n",
       "      <td>0.961585</td>\n",
       "      <td>0.930298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-06-12 12:06:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9387</td>\n",
       "      <td>190419_N04_0056_BHK37HDSXX</td>\n",
       "      <td>A3-35893</td>\n",
       "      <td>1~2~3~4</td>\n",
       "      <td>8</td>\n",
       "      <td>/data/seq2/seqIn//NovaSeq/190419_N04_0056_BHK3...</td>\n",
       "      <td>2019-04-24_15:58:27.9914912360_-0400</td>\n",
       "      <td>2019-04-19_15:53:06.0000000000_-0400</td>\n",
       "      <td>2019-04-24_15:58:32.6838125650_-0400</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>35.554067</td>\n",
       "      <td>0.954387</td>\n",
       "      <td>0.815699</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-06-12 12:06:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    x.X                 x.runFolder x.Sample_ID x.LaneNumbers  x.fastqCount  \\\n",
       "0  9341  190419_N03_0073_AHJF53DSXX    A3-35863       1~2~3~4             8   \n",
       "1  9342  190419_N03_0073_AHJF53DSXX    A3-35864       1~2~3~4             8   \n",
       "2  9343  190419_N03_0073_AHJF53DSXX    A3-35865       1~2~3~4             8   \n",
       "3  9344  190419_N03_0073_AHJF53DSXX    A3-35866       1~2~3~4             8   \n",
       "4  9387  190419_N04_0056_BHK37HDSXX    A3-35893       1~2~3~4             8   \n",
       "\n",
       "                                       x.sampleSheet  \\\n",
       "0  /data/seq2/seqIn//NovaSeq/190419_N03_0073_AHJF...   \n",
       "1  /data/seq2/seqIn//NovaSeq/190419_N03_0073_AHJF...   \n",
       "2  /data/seq2/seqIn//NovaSeq/190419_N03_0073_AHJF...   \n",
       "3  /data/seq2/seqIn//NovaSeq/190419_N03_0073_AHJF...   \n",
       "4  /data/seq2/seqIn//NovaSeq/190419_N04_0056_BHK3...   \n",
       "\n",
       "                              x.seqInSS                          x.seqInSSpre  \\\n",
       "0  2019-04-24_16:00:36.7968139030_-0400  2019-04-22_11:23:26.0000000000_-0400   \n",
       "1  2019-04-24_16:00:36.7968139030_-0400  2019-04-22_11:23:26.0000000000_-0400   \n",
       "2  2019-04-24_16:00:36.7968139030_-0400  2019-04-22_11:23:26.0000000000_-0400   \n",
       "3  2019-04-24_16:00:36.7968139030_-0400  2019-04-22_11:23:26.0000000000_-0400   \n",
       "4  2019-04-24_15:58:27.9914912360_-0400  2019-04-19_15:53:06.0000000000_-0400   \n",
       "\n",
       "                             x.seqOutSS  x.ssDiffFlag  ...  x.meanCoverageP  \\\n",
       "0  2019-04-24_16:00:41.3045690060_-0400             0  ...        45.614280   \n",
       "1  2019-04-24_16:00:41.3045690060_-0400             0  ...        43.295565   \n",
       "2  2019-04-24_16:00:41.3045690060_-0400             0  ...        40.469096   \n",
       "3  2019-04-24_16:00:41.3045690060_-0400             0  ...        42.829397   \n",
       "4  2019-04-24_15:58:32.6838125650_-0400             0  ...        35.554067   \n",
       "\n",
       "   x.PCT_20X  x.PCT_30X  x.NA. x.NA..1  x.NA..2  x.NA..3  x.NA..4  x.NA..5  \\\n",
       "0   0.963868   0.943429    NaN     NaN      NaN      NaN      NaN      NaN   \n",
       "1   0.962888   0.934077    NaN     NaN      NaN      NaN      NaN      NaN   \n",
       "2   0.960691   0.913079    NaN     NaN      NaN      NaN      NaN      NaN   \n",
       "3   0.961585   0.930298    NaN     NaN      NaN      NaN      NaN      NaN   \n",
       "4   0.954387   0.815699    NaN     NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "            x.valTime  \n",
       "0 2019-06-12 12:06:40  \n",
       "1 2019-06-12 12:06:40  \n",
       "2 2019-06-12 12:06:40  \n",
       "3 2019-06-12 12:06:40  \n",
       "4 2019-06-12 12:06:40  \n",
       "\n",
       "[5 rows x 97 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take a look at the USUHS QA table, if desired\n",
    "usuhs_qa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "#counts look right but check to see if any not found id\n",
    "print(set(fastqs_df['usuhsID']) - set(usuhs_keys_df['SampleID']))\n",
    "print(set(usuhs_keys_df['SampleID']) - set(fastqs_df['usuhsID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the fastqs and the key maps\n",
    "named_fastqs_df = pd.merge(fastqs_df,usuhs_keys_df,how='left',left_on='usuhsID',\\\n",
    "                           right_on='SampleID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1184, 5)\n",
      "(1184, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usuhsID</th>\n",
       "      <th>S</th>\n",
       "      <th>LANE</th>\n",
       "      <th>READ</th>\n",
       "      <th>NUM</th>\n",
       "      <th>SampleID</th>\n",
       "      <th>Flowcell</th>\n",
       "      <th>Description</th>\n",
       "      <th>oriID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3-35863</td>\n",
       "      <td>S21</td>\n",
       "      <td>L001</td>\n",
       "      <td>R1</td>\n",
       "      <td>1</td>\n",
       "      <td>A3-35863</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35863-PennEOPD001-A01</td>\n",
       "      <td>PennEOPD001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3-35863</td>\n",
       "      <td>S21</td>\n",
       "      <td>L001</td>\n",
       "      <td>R2</td>\n",
       "      <td>1</td>\n",
       "      <td>A3-35863</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35863-PennEOPD001-A01</td>\n",
       "      <td>PennEOPD001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3-35863</td>\n",
       "      <td>S21</td>\n",
       "      <td>L002</td>\n",
       "      <td>R1</td>\n",
       "      <td>1</td>\n",
       "      <td>A3-35863</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35863-PennEOPD001-A01</td>\n",
       "      <td>PennEOPD001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3-35863</td>\n",
       "      <td>S21</td>\n",
       "      <td>L002</td>\n",
       "      <td>R2</td>\n",
       "      <td>1</td>\n",
       "      <td>A3-35863</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35863-PennEOPD001-A01</td>\n",
       "      <td>PennEOPD001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3-35863</td>\n",
       "      <td>S21</td>\n",
       "      <td>L003</td>\n",
       "      <td>R1</td>\n",
       "      <td>1</td>\n",
       "      <td>A3-35863</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35863-PennEOPD001-A01</td>\n",
       "      <td>PennEOPD001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    usuhsID    S  LANE READ  NUM  SampleID                    Flowcell  \\\n",
       "0  A3-35863  S21  L001   R1    1  A3-35863  190419_N03_0073_AHJF53DSXX   \n",
       "1  A3-35863  S21  L001   R2    1  A3-35863  190419_N03_0073_AHJF53DSXX   \n",
       "2  A3-35863  S21  L002   R1    1  A3-35863  190419_N03_0073_AHJF53DSXX   \n",
       "3  A3-35863  S21  L002   R2    1  A3-35863  190419_N03_0073_AHJF53DSXX   \n",
       "4  A3-35863  S21  L003   R1    1  A3-35863  190419_N03_0073_AHJF53DSXX   \n",
       "\n",
       "                Description        oriID  \n",
       "0  A3-35863-PennEOPD001-A01  PennEOPD001  \n",
       "1  A3-35863-PennEOPD001-A01  PennEOPD001  \n",
       "2  A3-35863-PennEOPD001-A01  PennEOPD001  \n",
       "3  A3-35863-PennEOPD001-A01  PennEOPD001  \n",
       "4  A3-35863-PennEOPD001-A01  PennEOPD001  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(fastqs_df.shape)\n",
    "print(named_fastqs_df.shape)\n",
    "named_fastqs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1184\n",
      "Name: NUM, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#the NUM bit of the file name if always '001' so format the field\n",
    "print(named_fastqs_df['NUM'].value_counts())\n",
    "named_fastqs_df['NUM'] = '001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2    592\n",
       "R1    592\n",
       "Name: READ, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check exprect count R1 == R2\n",
    "named_fastqs_df['READ'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(592, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usuhsID</th>\n",
       "      <th>S</th>\n",
       "      <th>LANE</th>\n",
       "      <th>READ</th>\n",
       "      <th>NUM</th>\n",
       "      <th>SampleID</th>\n",
       "      <th>Flowcell</th>\n",
       "      <th>Description</th>\n",
       "      <th>oriID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3-35863</td>\n",
       "      <td>S21</td>\n",
       "      <td>L001</td>\n",
       "      <td>R1</td>\n",
       "      <td>001</td>\n",
       "      <td>A3-35863</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35863-PennEOPD001-A01</td>\n",
       "      <td>PennEOPD001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3-35863</td>\n",
       "      <td>S21</td>\n",
       "      <td>L002</td>\n",
       "      <td>R1</td>\n",
       "      <td>001</td>\n",
       "      <td>A3-35863</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35863-PennEOPD001-A01</td>\n",
       "      <td>PennEOPD001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3-35863</td>\n",
       "      <td>S21</td>\n",
       "      <td>L003</td>\n",
       "      <td>R1</td>\n",
       "      <td>001</td>\n",
       "      <td>A3-35863</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35863-PennEOPD001-A01</td>\n",
       "      <td>PennEOPD001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A3-35863</td>\n",
       "      <td>S21</td>\n",
       "      <td>L004</td>\n",
       "      <td>R1</td>\n",
       "      <td>001</td>\n",
       "      <td>A3-35863</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35863-PennEOPD001-A01</td>\n",
       "      <td>PennEOPD001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A3-35864</td>\n",
       "      <td>S22</td>\n",
       "      <td>L001</td>\n",
       "      <td>R1</td>\n",
       "      <td>001</td>\n",
       "      <td>A3-35864</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35864-PennEOPD002-B01</td>\n",
       "      <td>PennEOPD002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    usuhsID    S  LANE READ  NUM  SampleID                    Flowcell  \\\n",
       "0  A3-35863  S21  L001   R1  001  A3-35863  190419_N03_0073_AHJF53DSXX   \n",
       "2  A3-35863  S21  L002   R1  001  A3-35863  190419_N03_0073_AHJF53DSXX   \n",
       "4  A3-35863  S21  L003   R1  001  A3-35863  190419_N03_0073_AHJF53DSXX   \n",
       "6  A3-35863  S21  L004   R1  001  A3-35863  190419_N03_0073_AHJF53DSXX   \n",
       "8  A3-35864  S22  L001   R1  001  A3-35864  190419_N03_0073_AHJF53DSXX   \n",
       "\n",
       "                Description        oriID  \n",
       "0  A3-35863-PennEOPD001-A01  PennEOPD001  \n",
       "2  A3-35863-PennEOPD001-A01  PennEOPD001  \n",
       "4  A3-35863-PennEOPD001-A01  PennEOPD001  \n",
       "6  A3-35863-PennEOPD001-A01  PennEOPD001  \n",
       "8  A3-35864-PennEOPD002-B01  PennEOPD002  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only need one row for each sample, not both fastq pairs, ie R1 and R2\n",
    "cohort_df = named_fastqs_df.loc[named_fastqs_df['READ'] == 'R1']\n",
    "print(cohort_df.shape)\n",
    "cohort_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### format per sample jsons files for the fastq to ubam jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkfortemplatefile(this_template_file):\n",
    "    if not os.path.isfile(this_template_file):\n",
    "        print('need ' + this_template_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the jsons directory for the fastq to bam if it doesn't exist\n",
    "#also check to see if the blank jsons are present or you need to retrieve\n",
    "fastq_to_bam_json_dir = '{}/jsons'.format(WRKDIR)\n",
    "\n",
    "if os.path.isdir(fastq_to_bam_json_dir):\n",
    "    os.makedirs(fastq_to_bam_json_dir + '/fastqtoubam', exist_ok=True)    \n",
    "    \n",
    "checkfortemplatefile(fastq_to_bam_json_dir + '/blank.fastqtoubam.json')\n",
    "checkfortemplatefile(fastq_to_bam_json_dir + '/blank.broadbam.hg38.json')\n",
    "checkfortemplatefile(fastq_to_bam_json_dir + '/blank.align.label.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### here we are going to subet to just a 3rd or the sample, so each of you have a subset to process\n",
    "\n",
    "so you have to save you subset list to the file path below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 1)\n",
      "(592, 9)\n",
      "(192, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usuhsID</th>\n",
       "      <th>S</th>\n",
       "      <th>LANE</th>\n",
       "      <th>READ</th>\n",
       "      <th>NUM</th>\n",
       "      <th>SampleID</th>\n",
       "      <th>Flowcell</th>\n",
       "      <th>Description</th>\n",
       "      <th>oriID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3-35863</td>\n",
       "      <td>S21</td>\n",
       "      <td>L001</td>\n",
       "      <td>R1</td>\n",
       "      <td>001</td>\n",
       "      <td>A3-35863</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35863-PennEOPD001-A01</td>\n",
       "      <td>PennEOPD001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3-35863</td>\n",
       "      <td>S21</td>\n",
       "      <td>L002</td>\n",
       "      <td>R1</td>\n",
       "      <td>001</td>\n",
       "      <td>A3-35863</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35863-PennEOPD001-A01</td>\n",
       "      <td>PennEOPD001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3-35863</td>\n",
       "      <td>S21</td>\n",
       "      <td>L003</td>\n",
       "      <td>R1</td>\n",
       "      <td>001</td>\n",
       "      <td>A3-35863</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35863-PennEOPD001-A01</td>\n",
       "      <td>PennEOPD001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A3-35863</td>\n",
       "      <td>S21</td>\n",
       "      <td>L004</td>\n",
       "      <td>R1</td>\n",
       "      <td>001</td>\n",
       "      <td>A3-35863</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35863-PennEOPD001-A01</td>\n",
       "      <td>PennEOPD001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A3-35864</td>\n",
       "      <td>S22</td>\n",
       "      <td>L001</td>\n",
       "      <td>R1</td>\n",
       "      <td>001</td>\n",
       "      <td>A3-35864</td>\n",
       "      <td>190419_N03_0073_AHJF53DSXX</td>\n",
       "      <td>A3-35864-PennEOPD002-B01</td>\n",
       "      <td>PennEOPD002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    usuhsID    S  LANE READ  NUM  SampleID                    Flowcell  \\\n",
       "0  A3-35863  S21  L001   R1  001  A3-35863  190419_N03_0073_AHJF53DSXX   \n",
       "2  A3-35863  S21  L002   R1  001  A3-35863  190419_N03_0073_AHJF53DSXX   \n",
       "4  A3-35863  S21  L003   R1  001  A3-35863  190419_N03_0073_AHJF53DSXX   \n",
       "6  A3-35863  S21  L004   R1  001  A3-35863  190419_N03_0073_AHJF53DSXX   \n",
       "8  A3-35864  S22  L001   R1  001  A3-35864  190419_N03_0073_AHJF53DSXX   \n",
       "\n",
       "                Description        oriID  \n",
       "0  A3-35863-PennEOPD001-A01  PennEOPD001  \n",
       "2  A3-35863-PennEOPD001-A01  PennEOPD001  \n",
       "4  A3-35863-PennEOPD001-A01  PennEOPD001  \n",
       "6  A3-35863-PennEOPD001-A01  PennEOPD001  \n",
       "8  A3-35864-PennEOPD002-B01  PennEOPD002  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "your_subset_file = '{}/your_subset_of_full_cohort.list'.format(WRKDIR)\n",
    "your_subset = pd.read_csv(your_subset_file,header=None)\n",
    "print(your_subset.shape)\n",
    "\n",
    "print(cohort_df.shape)\n",
    "cohort_df = cohort_df.loc[cohort_df['oriID'].isin(your_subset[0])]\n",
    "print(cohort_df.shape)\n",
    "cohort_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format the WF input jsons for fastq to ubams\n",
    "from datetime import datetime\n",
    "\n",
    "json_template = '{}/jsons/blank.fastqtoubam.json'.format(WRKDIR)\n",
    "\n",
    "fastq_format = '{}/{}/fastqs/{}_{}_{}_{}_001.fastq.gz'\n",
    "\n",
    "DEFAULTATTEMPTS = 3\n",
    "DEFAULTDISK = 500\n",
    "DEFAULTMEM = '30 GB'\n",
    "\n",
    "sample_ids = cohort_df['oriID'].unique()\n",
    "for sample_id in sample_ids:\n",
    "    with open(json_template) as json_file:  \n",
    "        data = json.load(json_file)\n",
    "        data['ConvertPairedFastQsToUnmappedBamWf.PairedFastQsToUnmappedBAM.gatk_path'] = '/gatk/gatk'\n",
    "        data['ConvertPairedFastQsToUnmappedBamWf.PairedFastQsToUnmappedBAM.docker'] = 'broadinstitute/gatk:4.0.1.2'\n",
    "        data['ConvertPairedFastQsToUnmappedBamWf.PairedFastQsToUnmappedBAM.mem_size'] = DEFAULTMEM\n",
    "        data['ConvertPairedFastQsToUnmappedBamWf.PairedFastQsToUnmappedBAM.disk_size'] = DEFAULTDISK\n",
    "        data['ConvertPairedFastQsToUnmappedBamWf.PairedFastQsToUnmappedBAM.preemptible_tries'] = DEFAULTATTEMPTS\n",
    "\n",
    "        sample_data = data.copy()\n",
    "        sample_data['ConvertPairedFastQsToUnmappedBamWf.readgroup_list'] = []\n",
    "        sample_data['ConvertPairedFastQsToUnmappedBamWf.metadata'] = {}\n",
    "        sample_data['ConvertPairedFastQsToUnmappedBamWf.fastq_pairs'] = {}\n",
    "        sample_df = cohort_df.loc[cohort_df['oriID'] == sample_id]\n",
    "        json_outfile_name = '{}/jsons/fastqtoubam/{}.fastqtoubam.json'.format(WRKDIR,sample_id)\n",
    "\n",
    "        for index, row in sample_df.iterrows():\n",
    "            lib_date, lib_machine, lib_index, flowcell = row['Flowcell'].split('_')\n",
    "\n",
    "            read_group_name = '{}_{}_{}_{}'.format(row['oriID'], row['usuhsID'], row['LANE'], lib_date)\n",
    "\n",
    "            this_date_str = datetime.strptime(lib_date, '%y%m%d').strftime('%Y-%m-%d')\n",
    "            read_group_info = [row['oriID'], row['oriID'], row['Flowcell'], this_date_str, 'ILLUMINA', 'USUHS']\n",
    "\n",
    "            read_group_fastqs = [fastq_format.format(PRJ_BUCKET,COHORT,row['usuhsID'],row['S'],row['LANE'],'R1'), \\\n",
    "                                fastq_format.format(PRJ_BUCKET,COHORT,row['usuhsID'],row['S'],row['LANE'],'R2')]\n",
    "\n",
    "            sample_data['ConvertPairedFastQsToUnmappedBamWf.readgroup_list'].append(read_group_name)\n",
    "            sample_data['ConvertPairedFastQsToUnmappedBamWf.metadata'].update({read_group_name : read_group_info})\n",
    "            sample_data['ConvertPairedFastQsToUnmappedBamWf.fastq_pairs'].update({read_group_name : read_group_fastqs})\n",
    "        \n",
    "        with open(json_outfile_name,'w') as json_outfile:\n",
    "            json.dump(sample_data,json_outfile,sort_keys=True,indent=4)\n",
    "\n",
    "cohort_file_list = '{}/{}.samples.list'.format(WRKDIR,COHORT)\n",
    "pd.DataFrame(data=sample_ids).to_csv(cohort_file_list,header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#run these commands at terminal:\n",
      "\n",
      "chmod +x /labseq/projects/ninds_eopd/ninds_eopd.run_fastqs_to_ubams.sh\n",
      "nohup /labseq/projects/ninds_eopd/ninds_eopd.run_fastqs_to_ubams.sh > /labseq/projects/ninds_eopd/ninds_eopd.run_fastqs_to_ubams.log &\n"
     ]
    }
   ],
   "source": [
    "#define format function the ggp cmd\n",
    "def formatgcpcmd(this_sample,chrt_bucket):\n",
    "    this_cmd = 'echo -n {SAMPLE} OPID=\\n\\\n",
    "gcloud alpha genomics pipelines run \\\n",
    "--project {PROJECT_ID} \\\n",
    "--pipeline-file {PRJ_BUCKET}/resources/tools/wdl_pipeline.yaml \\\n",
    "--zones us-central1-f \\\n",
    "--memory 7 \\\n",
    "--logging {COHORT_BUCKET}/logs/ubams/{SAMPLE} \\\n",
    "--inputs-from-file WDL={WRKDIR}/tools/broad/paired-fastq-to-unmapped-bam.wdl \\\n",
    "--inputs-from-file WORKFLOW_INPUTS={WRKDIR}/jsons/{COHORT}/fastqtoubam/{SAMPLE}.fastqtoubam.json \\\n",
    "--inputs-from-file WORKFLOW_OPTIONS={WRKDIR}/jsons/generic.google-papi.options.json \\\n",
    "--inputs WORKSPACE={COHORT_BUCKET}/workspace/{SAMPLE} \\\n",
    "--inputs OUTPUTS={COHORT_BUCKET}/ubams/{SAMPLE} \\\n",
    "--labels=pipe=fastq_to_ubam,sample={LABELNAME},cohort={LCCOHORT},user={MYUSER}'\n",
    "    return(this_cmd.format(SAMPLE=this_sample,PROJECT_ID=PROJECT_ID,PRJ_BUCKET=PRJ_BUCKET,\\\n",
    "                         COHORT_BUCKET=chrt_bucket,WRKDIR=WRKDIR,COHORT=COHORT,\\\n",
    "                          LABELNAME=this_sample.lower(),LCCOHORT=COHORT.lower(),MYUSER=MYUSER))\n",
    "\n",
    "#iterate over samples formatting the cmds\n",
    "cohort_bucket = '{}/{}'.format(PRJ_BUCKET,COHORT)\n",
    "cmds = [formatgcpcmd(sample_id,cohort_bucket) for sample_id in sample_ids]\n",
    "\n",
    "temp_script_file = '{}/{}.run_fastqs_to_ubams.sh'.format(WRKDIR,COHORT.lower())\n",
    "\n",
    "with open(temp_script_file, 'w') as file_handler:\n",
    "        for this_cmd in cmds:\n",
    "            file_handler.write(\"{}\\n\".format(this_cmd))\n",
    "            \n",
    "print('#run these commands at terminal:\\n')\n",
    "print('chmod +x ' + temp_script_file)\n",
    "print('nohup ' + temp_script_file + ' > {}/{}.run_fastqs_to_ubams.log &'.format(WRKDIR,COHORT.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full job worker node count\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Listed 0 items.\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$PROJECT_ID\" \"$MYUSER\" \"$COHORT\"\n",
    "#see if there are instances running the job\n",
    "PROJECT_ID=${1}\n",
    "MYUSER=${2}\n",
    "COHORT=${3}\n",
    "\n",
    "PIPELABEL=fastq_to_ubam\n",
    "\n",
    "echo \"full job worker node count\"\n",
    "gcloud compute instances list --project ${PROJECT_ID} \\\n",
    "    --filter \"labels.pipe=${PIPELABEL} labels.cohort=${COHORT} labels.user=${MYUSER}\" | grep RUNNING | wc -l\n",
    "#echo \"job managers\"\n",
    "#gcloud compute instances list --project ${PROJECT_ID} \\\n",
    "#    --filter \"labels.pipe=${PIPELABEL} labels.cohort=${COHORT} labels.user=${MYUSER}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: true\n",
      "metadata:\n",
      "  events:\n",
      "  - description: start\n",
      "    startTime: '2019-05-23T17:17:17.141792399Z'\n",
      "  - description: pulling-image\n",
      "    startTime: '2019-05-23T17:17:17.141868185Z'\n",
      "  - description: localizing-files\n",
      "    startTime: '2019-05-23T17:17:53.776297803Z'\n",
      "  - description: running-docker\n",
      "    startTime: '2019-05-23T17:17:53.776333263Z'\n",
      "  - description: delocalizing-files\n",
      "    startTime: '2019-05-23T22:29:25.124150479Z'\n",
      "  - description: ok\n",
      "    startTime: '2019-05-23T22:29:26.216592166Z'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OPID=EI6syK2uLRjvoZHbvPnIuRgglfil2O0VKg9wcm9kdWN0aW9uUXVldWU\n",
    "gcloud alpha genomics operations describe ${OPID} \\\n",
    "--format='yaml(done, error, metadata.events)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$WRKDIR\" \"$PRJ_BUCKET\" \"$COHORT\"\n",
    "#get a list of files that were successfully created\n",
    "WRKDIR=${1}\n",
    "PRJ_BUCKET=${2}\n",
    "COHORT=${3}\n",
    "\n",
    "COHORT_BUCKET=${PRJ_BUCKET}/${COHORT}\n",
    "\n",
    "gsutil -mq ls ${COHORT_BUCKET}/ubams > ${WRKDIR}/${COHORT}.found.files\n",
    "\n",
    "sed -i s\"/gs:\\/\\/nihnialng-pd-wgs\\/${COHORT}\\/ubams\\///\"g ${WRKDIR}/${COHORT}.found.files\n",
    "sed -i s\"/\\///\"g ${WRKDIR}/${COHORT}.found.files\n",
    "\n",
    "less ${WRKDIR}/${COHORT}.found.files | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148, 1)\n",
      "(148, 1)\n",
      "0\n",
      "(0, 1)\n"
     ]
    }
   ],
   "source": [
    "#check for any missing expected bams\n",
    "expected_file = '{}/{}.samples.list'.format(WRKDIR, COHORT)\n",
    "observed_file = '{}/{}.found.files'.format(WRKDIR, COHORT) \n",
    "missing_file = '{}/{}.missing.samples.list'.format(WRKDIR, COHORT)\n",
    "\n",
    "expected = pd.read_csv(expected_file,header=None)\n",
    "observed = pd.read_csv(observed_file,header=None)\n",
    "\n",
    "print(expected.shape)\n",
    "print(observed.shape)\n",
    "\n",
    "print(len(set(expected[0]) - set(observed[0])))\n",
    "\n",
    "missing = expected.loc[~expected[0].isin(observed[0])]\n",
    "print(missing.shape)\n",
    "missing.head()\n",
    "\n",
    "#save the missing list\n",
    "missing.to_csv(missing_file,header=None,index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THIS IS WHERE WE STOPPED AFTER FASTQ TO uBAM coversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean up the temp workspace and logs from the fastq to ubam conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#run these commands at terminal:\n",
      "\n",
      "#WE'VE ALREADY RUN THIS SO DO NOT NEED TO CLEANUP AGAIN\n",
      "\n",
      "gsutil -mq rm -r gs://nihnialng-pd-wgs/ninds_eopd/logs/ubams\n",
      "gsutil -mq rm -r gs://nihnialng-pd-wgs/ninds_eopd/workspace\n"
     ]
    }
   ],
   "source": [
    "print('#run these commands at terminal:\\n')\n",
    "print('#WE\\'VE ALREADY RUN THIS SO DO NOT NEED TO CLEANUP AGAIN\\n')\n",
    "\n",
    "print('gsutil -mq rm -r {}/logs/ubams'.format(COHORT_BUCKET))\n",
    "print('gsutil -mq rm -r {}/workspace'.format(COHORT_BUCKET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make sure the json dir is there for the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the jsons directory for the ubam to cram if it doesn't exist\n",
    "#also check to see if the blank jsons are present or you need to retrieve\n",
    "fastq_to_bam_json_dir = '{}/jsons'.format(WRKDIR)\n",
    "\n",
    "if os.path.isdir(fastq_to_bam_json_dir):\n",
    "    os.makedirs(fastq_to_bam_json_dir + '/broadbams', exist_ok=True)    \n",
    "    \n",
    "checkfortemplatefile(fastq_to_bam_json_dir + '/blank.align.label.json')\n",
    "checkfortemplatefile(fastq_to_bam_json_dir + '/PairedEndSingleSampleWf.gatk4.0.options.json')\n",
    "checkfortemplatefile(fastq_to_bam_json_dir + '/template.broadbam.hg38.json')\n",
    "checkfortemplatefile(fastq_to_bam_json_dir + '/cromwell_client.py')\n",
    "checkfortemplatefile(fastq_to_bam_json_dir + '/PairedEndSingleSampleWf.gatk4.0.wdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#run these commands at terminal:\n",
      "\n",
      "gsutil cp gs://nihnialng-pd-wgs/tools/broad/blank.align.label.json /labseq/projects/ninds_eopd/jsons/\n",
      "gsutil cp gs://nihnialng-pd-wgs/tools/broad/PairedEndSingleSampleWf.gatk4.0.options.json /labseq/projects/ninds_eopd/jsons/\n",
      "gsutil cp gs://nihnialng-pd-wgs/tools/broad/template.broadbam.hg38.json /labseq/projects/ninds_eopd/jsons/\n",
      "gsutil cp gs://nihnialng-pd-wgs/tools/broad/json.substitute.py /labseq/projects/ninds_eopd/jsons/\n",
      "gsutil cp gs://nihnialng-pd-wgs/tools/broad/cromwell_client.py /labseq/projects/ninds_eopd/jsons/\n",
      "gsutil cp gs://nihnialng-pd-wgs/tools/broad/PairedEndSingleSampleWf.gatk4.0.wdl /labseq/projects/ninds_eopd/jsons/\n"
     ]
    }
   ],
   "source": [
    "print('#run these commands at terminal:\\n')\n",
    "\n",
    "print('gsutil cp gs://nihnialng-pd-wgs/tools/broad/blank.align.label.json {}/jsons/'.format(WRKDIR))\n",
    "print('gsutil cp gs://nihnialng-pd-wgs/tools/broad/PairedEndSingleSampleWf.gatk4.0.options.json \\\n",
    "{}/jsons/'.format(WRKDIR))\n",
    "print('gsutil cp gs://nihnialng-pd-wgs/tools/broad/template.broadbam.hg38.json {}/jsons/'.format(WRKDIR))\n",
    "print('gsutil cp gs://nihnialng-pd-wgs/tools/broad/cromwell_client.py {}/jsons/'.format(WRKDIR))\n",
    "print('gsutil cp gs://nihnialng-pd-wgs/tools/broad/PairedEndSingleSampleWf.gatk4.0.wdl {}/jsons/'.format(WRKDIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create the ubam to cram per sample json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the extra json files for ubams to crams; ie labels and options for alignment wf\n",
    "from datetime import datetime\n",
    "\n",
    "json_label_template = '{}/jsons/blank.align.label.json'.format(WRKDIR)\n",
    "json_options_template = '{}/jsons/PairedEndSingleSampleWf.gatk4.0.options.json'.format(WRKDIR)\n",
    "json_broad_template = '{}/jsons/template.broadbam.hg38.json'.format(WRKDIR)\n",
    "\n",
    "sample_ids = cohort_df['oriID'].unique()\n",
    "for sample_id in sample_ids:\n",
    "    json_labels_outfile_name = '{}/jsons/broadbams/{}.labels.json'.format(WRKDIR,sample_id)\n",
    "    json_options_outfile_name = '{}/jsons/broadbams/{}.options.json'.format(WRKDIR,sample_id)\n",
    "    json_broad_outfile_name = '{}/jsons/broadbams/{}.broadbam.hg38.json'.format(WRKDIR,sample_id)    \n",
    "\n",
    "    #format and write the label json\n",
    "    with open(json_label_template) as json_file:  \n",
    "        label_data = json.load(json_file)\n",
    "        \n",
    "        label_data['cohort'] = COHORT.lower()\n",
    "        label_data['sample'] = sample_id.lower()\n",
    "        label_data['user'] = MYUSER.lower()\n",
    "        \n",
    "        with open(json_labels_outfile_name,'w') as json_outfile:\n",
    "            json.dump(label_data,json_outfile,sort_keys=True,indent=4)   \n",
    "    \n",
    "    #format and write the options json\n",
    "    with open(json_options_template) as json_file:  \n",
    "        options_data = json.load(json_file)\n",
    "        \n",
    "        options_data['final_workflow_outputs_dir'] = '{}/hg38/align-wf/{}'.format(COHORT_BUCKET, sample_id)\n",
    "        options_data['final_workflow_log_dir'] = '{}/logs/{}'.format(COHORT_BUCKET, sample_id)\n",
    "        options_data['final_call_logs_dir'] = '{}/logs/{}'.format(COHORT_BUCKET, sample_id)\n",
    "        \n",
    "        with open(json_options_outfile_name,'w') as json_outfile:\n",
    "            json.dump(options_data,json_outfile,sort_keys=True,indent=4)   \n",
    "\n",
    "    #format and write the broad json\n",
    "    get_ubams_cmd = 'gsutil ls {}/ubams/{}/*.unmapped.bam'.format(COHORT_BUCKET,sample_id)\n",
    "    ubams = !{get_ubams_cmd}\n",
    "\n",
    "    with open(json_broad_template) as json_file:  \n",
    "        broad_data = json.load(json_file)\n",
    "        \n",
    "        broad_data['PairedEndSingleSampleWorkflow.sample_name'] = sample_id\n",
    "        broad_data['PairedEndSingleSampleWorkflow.base_file_name'] = sample_id\n",
    "        broad_data['PairedEndSingleSampleWorkflow.flowcell_unmapped_bams'] = ubams\n",
    "        broad_data['PairedEndSingleSampleWorkflow.final_gvcf_base_name'] = sample_id\n",
    "        \n",
    "        with open(json_broad_outfile_name,'w') as json_outfile:\n",
    "            json.dump(broad_data,json_outfile,sort_keys=False,indent=4)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setup the cromwell server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#setup a cromwell server for running the alignment wdl jobs\n",
      "#get Matt's cromwell stuff\n",
      "#already pulled for other projects so copy over from dementia_wgs\n",
      "\n",
      "#run these commands at terminal:\n",
      "\n",
      "#I'VE ALREADY RUN THIS, SO DO NOT NEED TO CREATE SERVER AGAIN\n",
      "\n",
      "#cp -r ../dementia_wgs/tools/verily-amp-pd-source /labseq/projects/ninds_eopd/tools/\n",
      "\n",
      "#fire up the cromwell instance\n",
      "chmod +x /labseq/projects/ninds_eopd/tools/verily-amp-pd-source/setup_cromwell_vm/*.sh\n",
      "cd /labseq/projects/ninds_eopd/tools/verily-amp-pd-source/setup_cromwell_vm/\n",
      "./create_cromwell_server.sh ninds_eopd-cromwell pd-genome n1-highmem-8\n",
      "./configure.sh ninds_eopd-cromwell pd-genome nihnialng-pd-wgs/ninds-eopd\n",
      "\n",
      "#When that is up, ssh to the instance:\n",
      "gcloud --project pd-genome compute ssh ninds-eopd-cromwell\n",
      "\n",
      "#And in that SSH session, run:\n",
      "cd /install\n",
      "docker-compose -f /install/workspace/config/docker-compose.yml up\n"
     ]
    }
   ],
   "source": [
    "print('#setup a cromwell server for running the alignment wdl jobs\\n\\\n",
    "#get Matt\\'s cromwell stuff\\n\\\n",
    "#already pulled for other projects so copy over from dementia_wgs\\n')\n",
    "\n",
    "print('#run these commands at terminal:\\n')\n",
    "print('#I\\'VE ALREADY RUN THIS, SO DO NOT NEED TO CREATE SERVER AGAIN\\n')\n",
    "print('#cp -r ../dementia_wgs/tools/verily-amp-pd-source {}/tools/'.format(WRKDIR))\n",
    "\n",
    "print('\\n#fire up the cromwell instance')\n",
    "print('chmod +x {}/tools/verily-amp-pd-source/setup_cromwell_vm/*.sh'.format(WRKDIR))\n",
    "print('cd {}/tools/verily-amp-pd-source/setup_cromwell_vm/'.format(WRKDIR))\n",
    "print('./create_cromwell_server.sh {}-cromwell {} n1-highmem-8'\\\n",
    "      .format(COHORT,PROJECT_ID))\n",
    "print('./configure.sh {}-cromwell {} nihnialng-pd-wgs/{}'\\\n",
    "      .format(COHORT,PROJECT_ID,COHORT.replace('_','-')))\n",
    "\n",
    "print('\\n#When that is up, ssh to the instance:')\n",
    "print('gcloud --project {} compute ssh {}-cromwell'.format(PROJECT_ID,COHORT.replace('_','-')))\n",
    "\n",
    "print('\\n#And in that SSH session, run:')\n",
    "print('cd /install')\n",
    "print('docker-compose -f /install/workspace/config/docker-compose.yml up')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create the ssh tunnel to the cromwell server so you can submit jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#When cromwell is up, create an SSH tunnel from your workstation:\n",
      "#run these commands at terminal:\n",
      "\n",
      "gcloud --project pd-genome compute ssh ninds-eopd-cromwell -- -L 8000:localhost:8000\n",
      "\n",
      "#after this runs you well actually be logged into the cromwell server, so you will need to open another termincal session on your machine to submit your jobs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('#When cromwell is up, create an SSH tunnel from your workstation:')\n",
    "print('#run these commands at terminal:\\n')\n",
    "print('gcloud --project {} compute ssh {}-cromwell -- -L 8000:localhost:8000'\\\n",
    "      .format(PROJECT_ID,COHORT.replace('_','-')))\n",
    "\n",
    "print('\\n#after this runs you well actually be logged into the cromwell server, ' \\\n",
    "      'so you will need to open another termincal session on your machine to submit your jobs\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#run these commands at terminal:\n",
      "\n",
      "chmod +x /labseq/projects/ninds_eopd/ninds_eopd.run_ubams_to_crams.sh\n",
      "nohup /labseq/projects/ninds_eopd/ninds_eopd.run_ubams_to_crams.sh > /labseq/projects/ninds_eopd/ninds_eopd.run_ubams_to_crams.log &\n"
     ]
    }
   ],
   "source": [
    "#define format function the ggp cmd\n",
    "def formatgcpcmd(this_sample,chrt_bucket):\n",
    "    this_cmd = 'echo -n {SAMPLE} OPID=\\n\\\n",
    "python {WRKDIR}/jsons/cromwell_client.py \\\n",
    "--wdl {WRKDIR}/jsons/PairedEndSingleSampleWf.gatk4.0.wdl \\\n",
    "--workflow-inputs {WRKDIR}/jsons/broadbams/{SAMPLE}.broadbam.hg38.json \\\n",
    "--workflow-options {WRKDIR}/jsons/broadbams/{SAMPLE}.options.json \\\n",
    "--workflow-labels {WRKDIR}/jsons/broadbams/{SAMPLE}.labels.json'\n",
    "    return(this_cmd.format(WRKDIR=WRKDIR,COHORT=COHORT,SAMPLE=this_sample))\n",
    "\n",
    "#iterate over samples formatting the cmds\n",
    "cohort_bucket = '{}/{}'.format(PRJ_BUCKET,COHORT)\n",
    "cmds = [formatgcpcmd(sample_id,cohort_bucket) for sample_id in sample_ids]\n",
    "\n",
    "temp_script_file = '{}/{}.run_ubams_to_crams.sh'.format(WRKDIR,COHORT.lower())\n",
    "\n",
    "with open(temp_script_file, 'w') as file_handler:\n",
    "        for this_cmd in cmds:\n",
    "            file_handler.write(\"{}\\n\".format(this_cmd))\n",
    "            \n",
    "print('#run these commands at terminal:\\n')\n",
    "print('chmod +x ' + temp_script_file)\n",
    "print('nohup ' + temp_script_file + ' > {}/{}.run_ubams_to_crams.log &'.format(WRKDIR,COHORT.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#full job worker node count\n",
      "Listed 0 items.\n",
      "0\n",
      "#number of all running instances in project\n",
      "676\n"
     ]
    }
   ],
   "source": [
    "#see how many GCE (Google Compute Engine) instances are running your jobs\n",
    "PIPELABEL='pairedendsinglesamplewf'\n",
    "\n",
    "print('#full job worker node count')\n",
    "!gcloud compute instances list --project {PROJECT_ID} \\\n",
    "--filter \"labels:({PIPELABEL} {COHORT} {MYUSER})\" | grep RUNNING | wc -l\n",
    "\n",
    "print('#number of all running instances in project')\n",
    "!gcloud compute instances list --project {PROJECT_ID} | grep RUNNING | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### commands for checking statuses using Cromwell REST cmds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#run these commands at terminal:\n",
      "\n",
      "#These actually won't be that helpful because you are all using the same server.\n",
      "\n",
      "#When cromwell is up, create an SSH tunnel from your workstation, if not already connected:\n",
      "\n",
      "gcloud --project pd-genome compute ssh ninds-eopd-cromwell -- -L 8000:localhost:8000\n",
      "\n",
      "#if tunnel established can check cromwell status\n",
      "\n",
      "curl -X GET \"http://localhost:8000/api/workflows/v1/query?status=Running\"\n",
      "curl -X GET \"http://localhost:8000/api/workflows/v1/query?status=Submitted\"\n",
      "curl -X GET \"http://localhost:8000/api/workflows/v1/query?status=Failed\"\n",
      "curl -X GET \"http://localhost:8000/api/workflows/v1/query?status=Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "print('#run these commands at terminal:\\n')\n",
    "\n",
    "print('#These actually won\\'t be that helpful because you are all using the same server.\\n')\n",
    "\n",
    "print('#When cromwell is up, create an SSH tunnel from your workstation, if not already connected:\\n')\n",
    "print('gcloud --project {} compute ssh {}-cromwell -- -L 8000:localhost:8000'.\\\n",
    "      format(PROJECT_ID,COHORT.replace('_','-')))\n",
    "\n",
    "print('\\n#if tunnel established can check cromwell status\\n')\n",
    "print('curl -X GET \"http://localhost:8000/api/workflows/v1/query?status=Running\"')\n",
    "print('curl -X GET \"http://localhost:8000/api/workflows/v1/query?status=Submitted\"')\n",
    "print('curl -X GET \"http://localhost:8000/api/workflows/v1/query?status=Failed\"')\n",
    "print('curl -X GET \"http://localhost:8000/api/workflows/v1/query?status=Succeeded\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### commands to check progess by counting expected output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#These actually won't be that helpful because you are all using the same bucket path.\n",
      "\n",
      "#crams\n",
      "118\n",
      "118\n",
      "#bams\n",
      "118\n",
      "118\n",
      "#gvcfs\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "#check bam, cram, and gvcf counts\n",
    "print('#These actually won\\'t be that helpful because you are all using the same bucket path.\\n')\n",
    "print('#crams')\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-ConvertToCram/**.cram | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-ConvertToCram/**.crai | wc -l\n",
    "print('#bams')\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-GatherBamFiles/**.bam | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-GatherBamFiles/**.bai | wc -l\n",
    "print('#gvcfs')\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-MergeVCFs/**.g.vcf.gz | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### commands for copying the primary output files to a final dest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#run these commands at terminal:\n",
      "\n",
      "#gvcfs\n",
      "gsutil -mq cp gs://nihnialng-pd-wgs/ninds_eopd/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-MergeVCFs/**.g.vcf.gz* gs://nihnialng-pd-wgs/ninds_eopd/hg38/gvcfs/\n",
      "\n",
      "#crams\n",
      "gsutil -mq cp gs://nihnialng-pd-wgs/ninds_eopd/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-ConvertToCram/**.cram* gs://nihnialng-pd-wgs/ninds_eopd/hg38/crams/\n",
      "\n",
      "#bams\n",
      "gsutil -mq cp gs://nihnialng-pd-wgs/ninds_eopd/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-GatherBamFiles/**.bam gs://nihnialng-pd-wgs/ninds_eopd/hg38/bams/\n",
      "gsutil -mq cp gs://nihnialng-pd-wgs/ninds_eopd/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-GatherBamFiles/**.bai gs://nihnialng-pd-wgs/ninds_eopd/hg38/bams/\n"
     ]
    }
   ],
   "source": [
    "print('#run these commands at terminal:\\n')\n",
    "\n",
    "print('#gvcfs')\n",
    "print('gsutil -mq cp {}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-MergeVCFs/**.g.vcf.gz* \\\n",
    "{}/hg38/gvcfs/'.format(COHORT_BUCKET,COHORT_BUCKET))\n",
    "print('\\n#crams')\n",
    "print('gsutil -mq cp {}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-ConvertToCram/**.cram* \\\n",
    "{}/hg38/crams/'.format(COHORT_BUCKET,COHORT_BUCKET))\n",
    "print('\\n#bams')\n",
    "print('gsutil -mq cp {}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-GatherBamFiles/**.bam \\\n",
    "{}/hg38/bams/'.format(COHORT_BUCKET,COHORT_BUCKET))\n",
    "print('gsutil -mq cp {}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-GatherBamFiles/**.bai \\\n",
    "{}/hg38/bams/'.format(COHORT_BUCKET,COHORT_BUCKET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confirm counts of the primary output files at final dest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check bam, cram, and gvcf counts at final path\n",
    "print('#crams')\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/crams/*.cram | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/crams/*.crai | wc -l\n",
    "print('#bams')\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/bams/*.bam | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/bams/*.bai | wc -l\n",
    "print('#gvcfs')\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/gvcfs/*.g.vcf.gz | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/gvcfs/*.g.vcf.gz.tbi | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check that all expected files are there by looking at sample IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$WRKDIR\" \"$COHORT_BUCKET\" \"$COHORT\"\n",
    "#get a list of files that were successfully created\n",
    "WRKDIR=${1}\n",
    "COHORT_BUCKET=${2}\n",
    "COHORT=${3}\n",
    "\n",
    "gsutil -mq ls ${COHORT_BUCKET}/hg38/crams/*.cram > ${WRKDIR}/${COHORT}.found.files\n",
    "\n",
    "sed -i s\"/gs:\\/\\/nihnialng-pd-wgs\\/${COHORT}\\/hg38\\/crams\\///\"g ${WRKDIR}/${COHORT}.found.files\n",
    "sed -i s\"/\\///\"g ${WRKDIR}/${COHORT}.found.files\n",
    "sed -i s\"/\\.cram//\"g ${WRKDIR}/${COHORT}.found.files\n",
    "\n",
    "less ${WRKDIR}/${COHORT}.found.files | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for any missing expected bams\n",
    "expected_file = '{}/{}.samples.list'.format(WRKDIR, COHORT)\n",
    "observed_file = '{}/{}.found.files'.format(WRKDIR, COHORT) \n",
    "missing_file = '{}/{}.missing.samples.list'.format(WRKDIR, COHORT)\n",
    "\n",
    "expected = pd.read_csv(expected_file,header=None)\n",
    "observed = pd.read_csv(observed_file,header=None)\n",
    "\n",
    "print(expected.shape)\n",
    "print(observed.shape)\n",
    "\n",
    "print(len(set(expected[0]) - set(observed[0])))\n",
    "\n",
    "missing = expected.loc[~expected[0].isin(observed[0])]\n",
    "print(missing.shape)\n",
    "missing.head()\n",
    "\n",
    "#save the missing list\n",
    "missing.to_csv(missing_file,header=None,index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if all the expected primary output files are present then go ahead and copy of the other output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#run these commands at terminal:\n",
      "\n",
      "gsutil -mq cp gs://nihnialng-pd-wgs/ninds_eopd/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-ValidateCram/**.cram.validation_report gs://nihnialng-pd-wgs/ninds_eopd/hg38/align-wf/call-ValidateCram/\n",
      "\n",
      "gsutil -mq cp $gs://nihnialng-pd-wgs/ninds_eopd/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-CheckContamination/**.preBqsr.selfSM gs://nihnialng-pd-wgs/ninds_eopd/hg38/align-wf/call-CheckContamination/\n",
      "\n",
      "gsutil -mq cp gs://nihnialng-pd-wgs/ninds_eopd/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-GatherBqsrReports/**.recal_data.csv gs://nihnialng-pd-wgs/ninds_eopd/hg38/align-wf/call-GatherBqsrReports/\n",
      "\n",
      "gsutil -mq cp gs://nihnialng-pd-wgs/ninds_eopd/hg38/align-wf/*/PairedEndSingleSampleWorkflow/**_metrics gs://nihnialng-pd-wgs/ninds_eopd/hg38/align-wf/metrics/\n",
      "\n",
      "gsutil -mq cp gs://nihnialng-pd-wgs/ninds_eopd/hg38/align-wf/*/PairedEndSingleSampleWorkflow/**.pdf gs://nihnialng-pd-wgs/ninds_eopd/hg38/align-wf/metrics/\n"
     ]
    }
   ],
   "source": [
    "#move the other metrics reports that may be of interest to keep\n",
    "#need to move all the other summary metric files over to final output\n",
    "print('#run these commands at terminal:\\n')\n",
    "print('gsutil -mq cp {}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-ValidateCram/**.cram.validation_report ' \\\n",
    "'{}/hg38/align-wf/call-ValidateCram/'.format(COHORT_BUCKET,COHORT_BUCKET))\n",
    "print('\\ngsutil -mq cp ${}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-CheckContamination/**.preBqsr.selfSM ' \\\n",
    "'{}/hg38/align-wf/call-CheckContamination/'.format(COHORT_BUCKET,COHORT_BUCKET))\n",
    "print('\\ngsutil -mq cp {}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-GatherBqsrReports/**.recal_data.csv ' \\\n",
    "'{}/hg38/align-wf/call-GatherBqsrReports/'.format(COHORT_BUCKET,COHORT_BUCKET))\n",
    "print('\\ngsutil -mq cp {}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/**_metrics {}/hg38/align-wf/metrics/'.\\\n",
    "      format(COHORT_BUCKET,COHORT_BUCKET))\n",
    "print('\\ngsutil -mq cp {}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/**.pdf {}/hg38/align-wf/metrics/'.\\\n",
    "      format(COHORT_BUCKET,COHORT_BUCKET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check file counts at destination for primary and other metrics output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n",
      "148\n",
      "148\n",
      "148\n",
      "148\n",
      "148\n",
      "148\n",
      "148\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "#check file counts\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/gvcfs/*.vcf.gz | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/gvcfs/*.vcf.gz.tbi | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/crams/*.cram | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/crams/*.crai | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/bams/*.bam | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/bams/*.bai | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/align-wf/call-ValidateCram/*.cram.validation_report | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/align-wf/call-CheckContamination/*.preBqsr.selfSM | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/align-wf/call-GatherBqsrReports/*.recal_data.csv | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check the cram validation reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "#pull down the validation reports and check for erros\n",
    "!mkdir -p {WRKDIR}/temp\n",
    "\n",
    "!gsutil -mq cp {COHORT_BUCKET}/hg38/align-wf/call-ValidateCram/*.cram.validation_report {WRKDIR}/temp/\n",
    "\n",
    "!ls {WRKDIR}/temp/*.validation_report | wc -l\n",
    "!less {WRKDIR}/temp/*.validation_report | grep \"No errors found\" | wc -l\n",
    "\n",
    "#clean up the temp validation reports\n",
    "!rm {WRKDIR}/temp/*.validation_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is where we stopped after alignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### do some additional cleanup, I've already run these deletes\n",
    "\n",
    "including the deletion of the ubams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsutil -mq rm -r gs://nihnialng-pd-wgs/ninds_eopd/logs\n",
      "gsutil -mq rm -r gs://nihnialng-pd-wgs/ninds_eopd/cromwell-execution/PairedEndSingleSampleWorkflow\n"
     ]
    }
   ],
   "source": [
    "#if successfully, clean up log files and workspace path\n",
    "!echo gsutil -mq rm -r {COHORT_BUCKET}/logs\n",
    "!echo gsutil -mq rm -r {COHORT_BUCKET}/cromwell-execution/PairedEndSingleSampleWorkflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#run these commands at terminal:\n",
      "\n",
      "#I'VE ALREADY RUN THIS SO DO NOT NEED TO DELETE AGAIN\n",
      "\n",
      "chmod +x /labseq/projects/ninds_eopd/ninds_eopd.delete_other_files.sh\n",
      "nohup /labseq/projects/ninds_eopd/ninds_eopd.delete_other_files.sh > /labseq/projects/ninds_eopd/ninds_eopd.delete_other_files.log &\n"
     ]
    }
   ],
   "source": [
    "#define format function the gcp cmd to delete additional per sample un-needed files\n",
    "def formatgcpcmd(this_sample,chrt_bucket):\n",
    "    this_cmd = 'gsutil -mq rm -r {COHORT_BUCKET}/hg38/align-wf/{SAMPLE}/PairedEndSingleSampleWorkflow'\n",
    "    return(this_cmd.format(COHORT_BUCKET=chrt_bucket,SAMPLE=this_sample))\n",
    "\n",
    "#here reloading sample_ids\n",
    "cohort_file_list = '{}/{}.samples.list'.format(WRKDIR,COHORT)\n",
    "sample_ids = pd.read_csv(cohort_file_list,header=None).to_numpy()\n",
    "sample_ids = sample_ids.reshape(len(sample_ids))\n",
    "\n",
    "#iterate over samples formatting the cmds\n",
    "cmds = [formatgcpcmd(sample_id,COHORT_BUCKET) for sample_id in sample_ids]\n",
    "\n",
    "temp_script_file = '{}/{}.delete_other_files.sh'.format(WRKDIR,COHORT.lower())\n",
    "\n",
    "with open(temp_script_file, 'w') as file_handler:\n",
    "        for this_cmd in cmds:\n",
    "            file_handler.write(\"{}\\n\".format(this_cmd))\n",
    "            \n",
    "print('#run these commands at terminal:\\n')\n",
    "print('#I\\'VE ALREADY RUN THIS SO DO NOT NEED TO DELETE AGAIN\\n')\n",
    "print('chmod +x ' + temp_script_file)\n",
    "print('nohup ' + temp_script_file + ' > {}/{}.delete_other_files.log &'.format(WRKDIR,COHORT.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#run these commands at terminal:\n",
      "\n",
      "#I'VE ALREADY RUN THIS SO DO NOT NEED TO DELETE AGAIN\n",
      "\n",
      "chmod +x /labseq/projects/ninds_eopd/ninds_eopd.delete_ubams.sh\n",
      "nohup /labseq/projects/ninds_eopd/ninds_eopd.delete_ubams.sh > /labseq/projects/ninds_eopd/ninds_eopd.delete_ubams.log &\n"
     ]
    }
   ],
   "source": [
    "#define format function the ggp cmd to delete ubams\n",
    "#deleting all the ubams (were just intermediate format)\n",
    "def formatgcpcmd(this_sample,chrt_bucket):\n",
    "    this_cmd = 'gsutil -mq rm -r {COHORT_BUCKET}/ubams/{SAMPLE}'\n",
    "    return(this_cmd.format(COHORT_BUCKET=chrt_bucket,SAMPLE=this_sample))\n",
    "\n",
    "#iterate over samples formatting the cmds\n",
    "cmds = [formatgcpcmd(sample_id,COHORT_BUCKET) for sample_id in sample_ids]\n",
    "\n",
    "temp_script_file = '{}/{}.delete_ubams.sh'.format(WRKDIR,COHORT.lower())\n",
    "\n",
    "with open(temp_script_file, 'w') as file_handler:\n",
    "        for this_cmd in cmds:\n",
    "            file_handler.write(\"{}\\n\".format(this_cmd))\n",
    "            \n",
    "print('#run these commands at terminal:\\n')\n",
    "print('#I\\'VE ALREADY RUN THIS SO DO NOT NEED TO DELETE AGAIN\\n')\n",
    "print('chmod +x ' + temp_script_file)\n",
    "print('nohup ' + temp_script_file + ' > {}/{}.delete_ubams.log &'.format(WRKDIR,COHORT.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### archive the fastqs to nearline storage for some periond of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#run these commands at terminal:\n",
      "\n",
      "#I'VE ALREADY RUN THIS SO DO NOT NEED TO MOVE AGAIN\n",
      "\n",
      "nohup gsutil -mq cp gs://nihnialng-pd-wgs/ninds_eopd/fastqs/*.fastq.gz gs://nihnialng-pd-archive/fastqs/usuhs/ninds_eopd_july2019/ &\n"
     ]
    }
   ],
   "source": [
    "#move fastqs to archvie bucket (nearline)\n",
    "#nearline, coldline -> father away storage bucket, cheaper \n",
    "fastq_bucket_path = '{}/{}/fastqs/*.fastq.gz'.format(PRJ_BUCKET,COHORT)\n",
    "archive_bucket_path = 'gs://nihnialng-pd-archive/fastqs/usuhs/{}/'.\\\n",
    "format(COHORTBUILD.replace('.', '_'))\n",
    "\n",
    "print('#run these commands at terminal:\\n')\n",
    "print('#I\\'VE ALREADY RUN THIS SO DO NOT NEED TO MOVE AGAIN\\n')\n",
    "print('nohup gsutil -mq cp {} {} &'.format(fastq_bucket_path, archive_bucket_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1184\n",
      "10.13 TiB    gs://nihnialng-pd-archive/fastqs/usuhs/ninds_eopd_july2019\n"
     ]
    }
   ],
   "source": [
    "#check that all the files were moved, although should be successful if no error was reported\n",
    "#should match the original counts from early fastq counting cell\n",
    "!gsutil ls {archive_bucket_path} | wc -l\n",
    "\n",
    "#get storage size\n",
    "!gsutil -mq du -hs {archive_bucket_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if everything is ok with archiving of fastqs, delete original staging bucket fastqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#run these commands at terminal:\n",
      "\n",
      "#I'VE ALREADY RUN THIS SO DO NOT NEED TO DELETE AGAIN\n",
      "\n",
      "gsutil -mq rm gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.00/*.fastq.gz\n",
      "gsutil -mq rm gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.01/*.fastq.gz\n",
      "gsutil -mq rm gs://nihnialng-pd-wgs/ninds_eopd/fastqs/*.fastq.gz\n"
     ]
    }
   ],
   "source": [
    "#delete the staging bucket fastqs that USUHS uploaded\n",
    "print('#run these commands at terminal:\\n')\n",
    "print('#I\\'VE ALREADY RUN THIS SO DO NOT NEED TO DELETE AGAIN\\n')\n",
    "print('gsutil -mq rm gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.00/*.fastq.gz')\n",
    "print('gsutil -mq rm gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.01/*.fastq.gz')\n",
    "print('gsutil -mq rm {}/fastqs/*.fastq.gz'.format(COHORT_BUCKET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check for any contaminated samples\n",
    "pull down seq contamination check info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n"
     ]
    }
   ],
   "source": [
    "#pull down the verifybamid check contamination files \n",
    "!mkdir -p {WRKDIR}/seqqc\n",
    "\n",
    "!gsutil -mq cp {COHORT_BUCKET}/hg38/align-wf/call-CheckContamination/*.preBqsr.selfSM {WRKDIR}/seqqc/\n",
    "!ls {WRKDIR}/seqqc/*.preBqsr.selfSM | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$WRKDIR\" \"$COHORTBUILD\" \"$COHORT\"\n",
    "#combine the contamination files into single table\n",
    "\n",
    "WRKDIR=${1}\n",
    "COHORTBUILD=${2}\n",
    "COHORT=${3}\n",
    "\n",
    "SAMPLESLISTFILE=\"${WRKDIR}/${COHORT}.samples.list\"\n",
    "METRICSFILE=\"${WRKDIR}/${COHORTBUILD}.contam.metrics.txt\"\n",
    "\n",
    "if [ -s ${METRICSFILE} ]; then\n",
    "rm ${METRICSFILE}\n",
    "fi\n",
    "echo \"id avgdp freemix\" > ${METRICSFILE}\n",
    "\n",
    "ls ${WRKDIR}/seqqc/*.preBqsr.selfSM | xargs -l basename > ${WRKDIR}/selfsm.file.list\n",
    "sed -i s\"/\\.preBqsr\\.selfSM//\"g ${WRKDIR}/selfsm.file.list\n",
    "\n",
    "while read SAMPLELINE\n",
    "do\n",
    "SAMPLE=$(echo ${SAMPLELINE} | awk '{print $1}')\n",
    "awk -v SAMPLE=${SAMPLE} '$1 == SAMPLE {print $1,$6,$7}' \\\n",
    "${WRKDIR}/seqqc/${SAMPLE}.preBqsr.selfSM >> ${METRICSFILE}\n",
    "done < ${WRKDIR}/selfsm.file.list\n",
    "\n",
    "rm ${WRKDIR}/selfsm.file.list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check the contamination reports for problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148, 3)\n",
      "number of samples maybe contaminated with freemix > 0.02 and < 0.049 is 0\n",
      "number of samples with freemix > 0.049 is 0\n",
      "number of samples with avgdp < 27 is 0\n"
     ]
    }
   ],
   "source": [
    "#check the contamination and seq coverage rates\n",
    "#reading report persample, lops over file loops into single table\n",
    "#lower than expected coverage, \n",
    "#2% and 4.9% = still useable?\n",
    "contam_metrics_file = '{}/{}.contam.metrics.txt'.format(WRKDIR,COHORTBUILD)\n",
    "\n",
    "WARN_FREEMIX = 0.02\n",
    "MAX_FREEMIX = 0.049\n",
    "MIN_DEPTH = 27\n",
    "\n",
    "metrics_df = pd.read_csv(contam_metrics_file,sep='\\s+')\n",
    "print(metrics_df.shape)\n",
    "metrics_df.head()\n",
    "\n",
    "maybe_contamin_df = metrics_df.loc[(metrics_df['freemix'] <= MAX_FREEMIX) & \\\n",
    "                                   (metrics_df['freemix'] > WARN_FREEMIX)]\n",
    "print('number of samples maybe contaminated with freemix > {} and < {} is {}'\\\n",
    "      .format(WARN_FREEMIX,MAX_FREEMIX,maybe_contamin_df.shape[0]))\n",
    "contam_maybe_file = '{}/{}.contamination.possible.samples.txt'.format(WRKDIR,COHORTBUILD)\n",
    "maybe_contamin_df.to_csv(contam_maybe_file,index=False,sep='\\t')\n",
    "\n",
    "contaminated_df = metrics_df.loc[metrics_df['freemix'] > MAX_FREEMIX]\n",
    "print('number of samples with freemix > {} is {}'.format(MAX_FREEMIX,contaminated_df.shape[0]))\n",
    "contam_problems_file = '{}/{}.contaminated.samples.txt'.format(WRKDIR,COHORTBUILD)\n",
    "contaminated_df.to_csv(contam_problems_file,index=False,sep='\\t')\n",
    "\n",
    "low_cov_df = metrics_df.loc[metrics_df['avgdp'] < MIN_DEPTH]\n",
    "print('number of samples with avgdp < {} is {}'.format(MIN_DEPTH,low_cov_df.shape[0]))\n",
    "low_coverage_file = '{}/{}.low_coverage.samples.txt'.format(WRKDIR,COHORTBUILD)\n",
    "low_cov_df.to_csv(low_coverage_file,index=False,sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### good news there aren't any samples with contamination or low coverage\n",
    "\n",
    "if there had been, you would want to just exclude the ones with freemix > 5% and decide on excluding the possibly contaminated or low coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now prep for running the joint genotyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pull down the broad tooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 33879  100 33879    0     0   827k      0 --:--:-- --:--:-- --:--:--  827k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  5493  100  5493    0     0   137k      0 --:--:-- --:--:-- --:--:--  137k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   134  100   134    0     0   1353      0 --:--:-- --:--:-- --:--:--  1353\n"
     ]
    }
   ],
   "source": [
    "#pull down the correct recent Broad tooling\n",
    "!mkdir -p {WRKDIR}/tools\n",
    "\n",
    "#don't think wget is easy to install for Mac, maybe use curl instead\n",
    "# !wget https://raw.githubusercontent.com/gatk-workflows/gatk4-germline-snps-indels/master/joint-discovery-gatk4.wdl \\\n",
    "#     -O {WRKDIR}/tools/joint-discovery-gatk4.wdl\n",
    "# !wget https://raw.githubusercontent.com/gatk-workflows/gatk4-germline-snps-indels/master/joint-discovery-gatk4.hg38.wgs.inputs.json \\\n",
    "#     -O {WRKDIR}/tools/joint-discovery-gatk4.hg38.wgs.inputs.json\n",
    "\n",
    "!curl https://raw.githubusercontent.com/gatk-workflows/gatk4-germline-snps-indels/master/joint-discovery-gatk4.wdl \\\n",
    "    -o {WRKDIR}/tools/joint-discovery-gatk4.wdl\n",
    "!curl https://raw.githubusercontent.com/gatk-workflows/gatk4-germline-snps-indels/master/joint-discovery-gatk4.hg38.wgs.inputs.json \\\n",
    "    -o {WRKDIR}/tools/joint-discovery-gatk4.hg38.wgs.inputs.json\n",
    "    \n",
    "!curl https://raw.githubusercontent.com/gatk-workflows/gatk4-germline-snps-indels/master/generic.google-papi.options.json \\\n",
    "    -o {WRKDIR}/tools/generic.google-papi.options.json    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### update joint calling jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update some of the json configs for broad joint calling\n",
    "#generate the extra json files for ubams to crams; ie labels and options for alignment wf\n",
    "#formats set of json files that formats files were running\n",
    "\n",
    "generic_options_template = '{}/tools/generic.google-papi.options.json'.format(WRKDIR)\n",
    "wdl_input_template = '{}/tools/joint-discovery-gatk4.hg38.wgs.inputs.json'.format(WRKDIR)\n",
    "\n",
    "json_labels_outfile_name = '{}/jsons/{}.jdgatk4.labels.json'.format(WRKDIR,COHORT)\n",
    "json_options_outfile_name = '{}/jsons/{}.jdgatk4.options.json'.format(WRKDIR,COHORT)\n",
    "json_broad_outfile_name = '{}/jsons/{}.jdgatk4.hg38.wgs.inputs.json'.format(WRKDIR,COHORT)    \n",
    "\n",
    "#format and write the label json\n",
    "label_data = {}\n",
    "label_data['workflow'] = 'jdgatk4'\n",
    "label_data['cohort'] = COHORT.lower()\n",
    "label_data['user'] = MYUSER.lower()\n",
    "\n",
    "with open(json_labels_outfile_name,'w') as json_outfile:\n",
    "    json.dump(label_data,json_outfile,sort_keys=True,indent=4)   \n",
    "\n",
    "#format and write the options json\n",
    "with open(generic_options_template) as json_file:  \n",
    "    options_data = json.load(json_file)\n",
    "    \n",
    "    options_data['read_from_cache'] = True\n",
    "    options_data['write_to_cache'] = True\n",
    "    options_data['workflow_failure_mode'] = 'ContinueWhilePossible'\n",
    "    options_data['system.input-read-limits.lines'] = 640000    \n",
    "    options_data['final_workflow_outputs_dir'] = '{}/hg38/joint_calling'.format(COHORT_BUCKET)\n",
    "    options_data['final_workflow_log_dir'] = '{}/logs/joint_calling'.format(COHORT_BUCKET)\n",
    "    options_data['final_call_logs_dir'] = '{}/logs/joint_calling'.format(COHORT_BUCKET)\n",
    "\n",
    "    with open(json_options_outfile_name,'w') as json_outfile:\n",
    "        json.dump(options_data,json_outfile,sort_keys=True,indent=4)   \n",
    "\n",
    "#format and write the broad json\n",
    "with open(wdl_input_template) as json_file:  \n",
    "    broad_data = json.load(json_file)\n",
    "    \n",
    "    sample_gvcfs_path = '{}/{}.sample.gvcfs.map.txt'.format(COHORT_BUCKET,COHORTBUILD)\n",
    "    broad_data['JointGenotyping.callset_name'] = COHORTBUILD\n",
    "    broad_data['JointGenotyping.sample_name_map'] = sample_gvcfs_path\n",
    "\n",
    "    with open(json_broad_outfile_name,'w') as json_outfile:\n",
    "        json.dump(broad_data,json_outfile,sort_keys=False,indent=4)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$WRKDIR\" \"$COHORT_BUCKET\" \"$COHORT\"\n",
    "#get a list of files that were successfully created\n",
    "WRKDIR=${1}\n",
    "COHORT_BUCKET=${2}\n",
    "COHORT=${3}\n",
    "\n",
    "gsutil -mq ls ${COHORT_BUCKET}/hg38/crams/*.cram > ${WRKDIR}/${COHORT}.found.files\n",
    "\n",
    "sed -i s\"/gs:\\/\\/nihnialng-pd-wgs\\/${COHORT}\\/hg38\\/crams\\///\"g ${WRKDIR}/${COHORT}.found.files\n",
    "sed -i s\"/\\///\"g ${WRKDIR}/${COHORT}.found.files\n",
    "sed -i s\"/\\.cram//\"g ${WRKDIR}/${COHORT}.found.files\n",
    "\n",
    "less ${WRKDIR}/${COHORT}.found.files | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to create the gvcfs sample map file used as json argument above JointGenotyping.sample_name_map\n",
    "#instead of just looping expected ids do so from found list in previous cell, allow to move/remove fails\n",
    "\n",
    "found_file_list = '{}/{}.found.files'.format(WRKDIR,COHORT)\n",
    "found_ids = pd.read_csv(found_file_list,header=None).to_numpy()\n",
    "found_ids = sample_ids.reshape(len(found_ids))\n",
    "\n",
    "gvcfs_map_file = '{}/{}.sample.gvcfs.map.txt'.format(WRKDIR,COHORTBUILD)\n",
    "\n",
    "with open(gvcfs_map_file, 'w') as file_handler:\n",
    "    for sample_id in found_ids:\n",
    "        file_handler.write(\"{}\\t{}/hg38/gvcfs/{}.g.vcf.gz\\n\".\\\n",
    "                           format(sample_id,COHORT_BUCKET,sample_id))\n",
    "\n",
    "#now copy the map file up to cloud bucket\n",
    "!gsutil -mq cp {gvcfs_map_file} {COHORT_BUCKET}/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now run the joint calling job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#When cromwell is up, create an SSH tunnel from your workstation:\n",
      "#run these commands at terminal:\n",
      "\n",
      "gcloud --project pd-genome compute ssh ninds-eopd-cromwell -- -L 8000:localhost:8000\n",
      "\n",
      "#after this runs you well actually be logged into the cromwell server, so you will need to open another termincal session on your machine to submit your jobs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('#When cromwell is up, create an SSH tunnel from your workstation:')\n",
    "print('#run these commands at terminal:\\n')\n",
    "print('gcloud --project {} compute ssh {}-cromwell -- -L 8000:localhost:8000'\\\n",
    "      .format(PROJECT_ID,COHORT.replace('_','-')))\n",
    "\n",
    "print('\\n#after this runs you well actually be logged into the cromwell server, ' \\\n",
    "      'so you will need to open another termincal session on your machine to submit your jobs\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#run these commands at terminal:\n",
      "\n",
      "python /labseq/projects/ninds_eopd/jsons/cromwell_client.py --wdl /labseq/projects/ninds_eopd/tools/oint-discovery-gatk4.wdl --workflow-inputs /labseq/projects/ninds_eopd/jsons/ninds_eopd.jdgatk4.hg38.wgs.inputs.json --workflow-options /labseq/projects/ninds_eopd/jsons/ninds_eopd.jdgatk4.options.json --workflow-labels /labseq/projects/ninds_eopd/jsons/ninds_eopd.jdgatk4.labels.json > /labseq/projects/ninds_eopd/ninds_eopd.july2019.jdgatk4.jobid\n"
     ]
    }
   ],
   "source": [
    "#now run the joint calling job, submit job\n",
    "\n",
    "run_cmd = 'python {wrk_dir}/jsons/cromwell_client.py \\\n",
    "--wdl {wrk_dir}/tools/oint-discovery-gatk4.wdl \\\n",
    "--workflow-inputs {wrk_dir}/jsons/{this_cohort}.jdgatk4.hg38.wgs.inputs.json \\\n",
    "--workflow-options {wrk_dir}/jsons/{this_cohort}.jdgatk4.options.json \\\n",
    "--workflow-labels {wrk_dir}/jsons/{this_cohort}.jdgatk4.labels.json'.\\\n",
    "format(wrk_dir=WRKDIR, this_cohort=COHORT)\n",
    "\n",
    "print('#run these commands at terminal:\\n')\n",
    "\n",
    "print('{} > {}/{}.jdgatk4.jobid'.format(run_cmd, WRKDIR, COHORTBUILD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are stopping here until next meeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcloud compute instances delete ninds-eopd-cromwell --project pd-genome --zone us-central1-f\n"
     ]
    }
   ],
   "source": [
    "#if done with the cromwell server delete it\n",
    "\n",
    "!echo gcloud compute instances delete {COHORT.replace('_','-')}-cromwell \\\n",
    "--project {PROJECT_ID} --zone us-central1-f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
