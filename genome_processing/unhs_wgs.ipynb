{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2001UNHS-0041 WGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### February 7, 2020\n",
    "\n",
    "### notebook for processing the UNHS WGS data\n",
    "#### based on script from Raph Gibbs (CBG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### global varibles and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global notebook variables for both python and bash majic (by stdin arguments)\n",
    "# WRKDIR = '/Users/mooreank/Desktop/WGS'\n",
    "# PRJ_BUCKET = 'gs://nihnialng-pd-wgs'\n",
    "# PROJECT_ID = 'pd-genome'\n",
    "# MYUSER = 'mooreank'\n",
    "# AUTOSOMES=[str(x) for x in list(range(1,23))]\n",
    "# SEXOMES=['X','Y']\n",
    "# CHROMOSOMES=AUTOSOMES + SEXOMES\n",
    "# COHORT='ninds_eopd'\n",
    "# COHORTBUILD='{}.july2019'.format(COHORT)\n",
    "# COHORT_BUCKET='{}/{}'.format(PRJ_BUCKET,COHORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRKDIR = '/labshare/anni/UNHS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### see what fastqs are present in the stagin bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#staging bucket that USUHS uploaded fastqs to\n",
    "!ls /labshare/anni/UNHS/2001UNHS-0041/ | wc -l\n",
    "# ls gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.01 | wc -l\n",
    "    \n",
    "# ls gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.00 | head\n",
    "# ls gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.01 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_bucket_path = '{}/{}/fastqs'.format(PRJ_BUCKET,COHORT)\n",
    "gcs_fastq_mv_cmd = 'gsutil -mq cp gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.0*/*.fastq.gz \\\n",
    "{}/'.format(dest_bucket_path)\n",
    "\n",
    "print('#run these commands at terminal:\\n')\n",
    "print('#WE\\'VE ALREADY RUN THIS SO DO NOT NEED TO COPY AGAIN\\n')\n",
    "print(gcs_fastq_mv_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$dest_bucket_path\"\n",
    "#check all the fastqs have been moved should be 592 pairs\n",
    "DEST_BUCKET=${1}\n",
    "\n",
    "gsutil ls ${DEST_BUCKET}/*_R1_001.fastq.gz | wc -l\n",
    "gsutil ls ${DEST_BUCKET}/*_R2_001.fastq.gz | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get fastqs listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$WRKDIR\" \"$dest_bucket_path\" \"$COHORT\"\n",
    "#get fastq listing and tokenize\n",
    "WRKDIR=${1}\n",
    "DEST_BUCKET=${2}\n",
    "COHORT=${3}\n",
    "\n",
    "FASTQ_LISTING=${WRKDIR}/${COHORT}.fastq.listing.txt\n",
    "\n",
    "gsutil ls ${DEST_BUCKET}/*.fastq.gz > ${FASTQ_LISTING}\n",
    "\n",
    "sed -i s\"/gs:\\/\\/nihnialng-pd-wgs\\/${COHORT}\\/fastqs\\///\"g ${FASTQ_LISTING}\n",
    "sed -i s\"/\\.fastq\\.gz//\"g ${FASTQ_LISTING}\n",
    "sed -i s\"/_/\\\\t/\"g ${FASTQ_LISTING}\n",
    "\n",
    "less ${FASTQ_LISTING} | wc -l\n",
    "head ${FASTQ_LISTING}\n",
    "tail ${FASTQ_LISTING}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load fastq listing and USUHS sample info and ID mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok now see what we have\n",
    "#load the USUHS quality reports\n",
    "usuhs_qa_file = WRKDIR + '/pA3.QA.june.26.2019.xlsx'\n",
    "usuhs_qa = pd.read_excel(usuhs_qa_file)\n",
    "print(usuhs_qa.shape)\n",
    "\n",
    "#load the fastq info\n",
    "fastqs_file = '{}/{}.fastq.listing.txt'.format(WRKDIR,COHORT)\n",
    "fastqs_df = pd.read_csv(fastqs_file,header=None,sep='\\t')\n",
    "fastqs_df.columns = ['usuhsID','S','LANE','READ','NUM']\n",
    "print(fastqs_df.shape)\n",
    "\n",
    "#load usuhs key map\n",
    "usuhs_key_file = WRKDIR + '/pA3.hernandez.export1.keyTable.june.27.2019.csv'\n",
    "usuhs_keys_df = pd.read_csv(usuhs_key_file,sep=',')\n",
    "print(usuhs_keys_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(usuhs_keys_df.shape)\n",
    "usuhs_keys_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unfortunately of 'real' sampleID is packed in the middle of the 'Description' column values, extract it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usuhs_keys_df['grbg1'],usuhs_keys_df['grbg2'],usuhs_keys_df['oriID'],usuhs_keys_df['grbg3'] = \\\n",
    "usuhs_keys_df['Description'].str.split('-').str\n",
    "usuhs_keys_df.drop(['grbg1','grbg2','grbg3'],axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a look at the USUHS QA table, if desired\n",
    "usuhs_qa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts look right but check to see if any not found id\n",
    "print(set(fastqs_df['usuhsID']) - set(usuhs_keys_df['SampleID']))\n",
    "print(set(usuhs_keys_df['SampleID']) - set(fastqs_df['usuhsID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the fastqs and the key maps\n",
    "named_fastqs_df = pd.merge(fastqs_df,usuhs_keys_df,how='left',left_on='usuhsID',\\\n",
    "                           right_on='SampleID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fastqs_df.shape)\n",
    "print(named_fastqs_df.shape)\n",
    "named_fastqs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the NUM bit of the file name if always '001' so format the field\n",
    "print(named_fastqs_df['NUM'].value_counts())\n",
    "named_fastqs_df['NUM'] = '001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check exprect count R1 == R2\n",
    "named_fastqs_df['READ'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only need one row for each sample, not both fastq pairs, ie R1 and R2\n",
    "cohort_df = named_fastqs_df.loc[named_fastqs_df['READ'] == 'R1']\n",
    "print(cohort_df.shape)\n",
    "cohort_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### format per sample jsons files for the fastq to ubam jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkfortemplatefile(this_template_file):\n",
    "    if not os.path.isfile(this_template_file):\n",
    "        print('need ' + this_template_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the jsons directory for the fastq to bam if it doesn't exist\n",
    "#also check to see if the blank jsons are present or you need to retrieve\n",
    "fastq_to_bam_json_dir = '{}/jsons'.format(WRKDIR)\n",
    "\n",
    "if os.path.isdir(fastq_to_bam_json_dir):\n",
    "    os.makedirs(fastq_to_bam_json_dir + '/fastqtoubam', exist_ok=True)    \n",
    "    \n",
    "checkfortemplatefile(fastq_to_bam_json_dir + '/blank.fastqtoubam.json')\n",
    "checkfortemplatefile(fastq_to_bam_json_dir + '/blank.broadbam.hg38.json')\n",
    "checkfortemplatefile(fastq_to_bam_json_dir + '/blank.align.label.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### here we are going to subet to just a 3rd or the sample, so each of you have a subset to process\n",
    "\n",
    "so you have to save you subset list to the file path below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_subset_file = '{}/anni_subset_of_full_cohort.list'.format(WRKDIR)\n",
    "your_subset = pd.read_csv(your_subset_file,header=None)\n",
    "print(your_subset.shape)\n",
    "\n",
    "print(cohort_df.shape)\n",
    "cohort_df = cohort_df.loc[cohort_df['oriID'].isin(your_subset[0])]\n",
    "print(cohort_df.shape)\n",
    "cohort_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format the WF input jsons for fastq to ubams\n",
    "from datetime import datetime\n",
    "\n",
    "json_template = '{}/jsons/blank.fastqtoubam.json'.format(WRKDIR)\n",
    "\n",
    "fastq_format = '{}/{}/fastqs/{}_{}_{}_{}_001.fastq.gz'\n",
    "\n",
    "DEFAULTATTEMPTS = 3\n",
    "DEFAULTDISK = 500\n",
    "DEFAULTMEM = '30 GB'\n",
    "\n",
    "sample_ids = cohort_df['oriID'].unique()\n",
    "for sample_id in sample_ids:\n",
    "    with open(json_template) as json_file:  \n",
    "        data = json.load(json_file)\n",
    "        data['ConvertPairedFastQsToUnmappedBamWf.PairedFastQsToUnmappedBAM.gatk_path'] = '/gatk/gatk'\n",
    "        data['ConvertPairedFastQsToUnmappedBamWf.PairedFastQsToUnmappedBAM.docker'] = 'broadinstitute/gatk:4.0.1.2'\n",
    "        data['ConvertPairedFastQsToUnmappedBamWf.PairedFastQsToUnmappedBAM.mem_size'] = DEFAULTMEM\n",
    "        data['ConvertPairedFastQsToUnmappedBamWf.PairedFastQsToUnmappedBAM.disk_size'] = DEFAULTDISK\n",
    "        data['ConvertPairedFastQsToUnmappedBamWf.PairedFastQsToUnmappedBAM.preemptible_tries'] = DEFAULTATTEMPTS\n",
    "\n",
    "        sample_data = data.copy()\n",
    "        sample_data['ConvertPairedFastQsToUnmappedBamWf.readgroup_list'] = []\n",
    "        sample_data['ConvertPairedFastQsToUnmappedBamWf.metadata'] = {}\n",
    "        sample_data['ConvertPairedFastQsToUnmappedBamWf.fastq_pairs'] = {}\n",
    "        sample_df = cohort_df.loc[cohort_df['oriID'] == sample_id]\n",
    "        json_outfile_name = '{}/jsons/fastqtoubam/{}.fastqtoubam.json'.format(WRKDIR,sample_id)\n",
    "\n",
    "        for index, row in sample_df.iterrows():\n",
    "            lib_date, lib_machine, lib_index, flowcell = row['Flowcell'].split('_')\n",
    "\n",
    "            read_group_name = '{}_{}_{}_{}'.format(row['oriID'], row['usuhsID'], row['LANE'], lib_date)\n",
    "\n",
    "            this_date_str = datetime.strptime(lib_date, '%y%m%d').strftime('%Y-%m-%d')\n",
    "            read_group_info = [row['oriID'], row['oriID'], row['Flowcell'], this_date_str, 'ILLUMINA', 'USUHS']\n",
    "\n",
    "            read_group_fastqs = [fastq_format.format(PRJ_BUCKET,COHORT,row['usuhsID'],row['S'],row['LANE'],'R1'), \\\n",
    "                                fastq_format.format(PRJ_BUCKET,COHORT,row['usuhsID'],row['S'],row['LANE'],'R2')]\n",
    "\n",
    "            sample_data['ConvertPairedFastQsToUnmappedBamWf.readgroup_list'].append(read_group_name)\n",
    "            sample_data['ConvertPairedFastQsToUnmappedBamWf.metadata'].update({read_group_name : read_group_info})\n",
    "            sample_data['ConvertPairedFastQsToUnmappedBamWf.fastq_pairs'].update({read_group_name : read_group_fastqs})\n",
    "        \n",
    "        with open(json_outfile_name,'w') as json_outfile:\n",
    "            json.dump(sample_data,json_outfile,sort_keys=True,indent=4)\n",
    "\n",
    "cohort_file_list = '{}/{}.samples.list'.format(WRKDIR,COHORT)\n",
    "pd.DataFrame(data=sample_ids).to_csv(cohort_file_list,header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define format function the ggp cmd\n",
    "def formatgcpcmd(this_sample,chrt_bucket):\n",
    "    this_cmd = 'echo -n {SAMPLE} OPID=\\n\\\n",
    "gcloud alpha genomics pipelines run \\\n",
    "--project {PROJECT_ID} \\\n",
    "--pipeline-file {PRJ_BUCKET}/resources/tools/wdl_pipeline.yaml \\\n",
    "--zones us-central1-f \\\n",
    "--memory 7 \\\n",
    "--logging {COHORT_BUCKET}/logs/ubams/{SAMPLE} \\\n",
    "--inputs-from-file WDL={WRKDIR}/tools/broad/paired-fastq-to-unmapped-bam.wdl \\\n",
    "--inputs-from-file WORKFLOW_INPUTS={WRKDIR}/jsons/{COHORT}/fastqtoubam/{SAMPLE}.fastqtoubam.json \\\n",
    "--inputs-from-file WORKFLOW_OPTIONS={WRKDIR}/jsons/generic.google-papi.options.json \\\n",
    "--inputs WORKSPACE={COHORT_BUCKET}/workspace/{SAMPLE} \\\n",
    "--inputs OUTPUTS={COHORT_BUCKET}/ubams/{SAMPLE} \\\n",
    "--labels=pipe=fastq_to_ubam,sample={LABELNAME},cohort={LCCOHORT},user={MYUSER}'\n",
    "    return(this_cmd.format(SAMPLE=this_sample,PROJECT_ID=PROJECT_ID,PRJ_BUCKET=PRJ_BUCKET,\\\n",
    "                         COHORT_BUCKET=chrt_bucket,WRKDIR=WRKDIR,COHORT=COHORT,\\\n",
    "                          LABELNAME=this_sample.lower(),LCCOHORT=COHORT.lower(),MYUSER=MYUSER))\n",
    "\n",
    "#iterate over samples formatting the cmds\n",
    "cohort_bucket = '{}/{}'.format(PRJ_BUCKET,COHORT)\n",
    "cmds = [formatgcpcmd(sample_id,cohort_bucket) for sample_id in sample_ids]\n",
    "\n",
    "temp_script_file = '{}/{}.run_fastqs_to_ubams.sh'.format(WRKDIR,COHORT.lower())\n",
    "\n",
    "with open(temp_script_file, 'w') as file_handler:\n",
    "        for this_cmd in cmds:\n",
    "            file_handler.write(\"{}\\n\".format(this_cmd))\n",
    "            \n",
    "print('#run these commands at terminal:\\n')\n",
    "print('chmod +x ' + temp_script_file)\n",
    "print('nohup ' + temp_script_file + ' > {}/{}.run_fastqs_to_ubams.log &'.format(WRKDIR,COHORT.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$PROJECT_ID\" \"$MYUSER\" \"$COHORT\"\n",
    "#see if there are instances running the job\n",
    "PROJECT_ID=${1}\n",
    "MYUSER=${2}\n",
    "COHORT=${3}\n",
    "\n",
    "PIPELABEL=fastq_to_ubam\n",
    "\n",
    "echo \"full job worker node count\"\n",
    "gcloud compute instances list --project ${PROJECT_ID} \\\n",
    "    --filter \"labels.pipe=${PIPELABEL} labels.cohort=${COHORT} labels.user=${MYUSER}\" | grep RUNNING | wc -l\n",
    "#echo \"job managers\"\n",
    "#gcloud compute instances list --project ${PROJECT_ID} \\\n",
    "#    --filter \"labels.pipe=${PIPELABEL} labels.cohort=${COHORT} labels.user=${MYUSER}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "OPID=EI6syK2uLRjvoZHbvPnIuRgglfil2O0VKg9wcm9kdWN0aW9uUXVldWU\n",
    "gcloud alpha genomics operations describe ${OPID} \\\n",
    "--format='yaml(done, error, metadata.events)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$WRKDIR\" \"$PRJ_BUCKET\" \"$COHORT\"\n",
    "#get a list of files that were successfully created\n",
    "WRKDIR=${1}\n",
    "PRJ_BUCKET=${2}\n",
    "COHORT=${3}\n",
    "\n",
    "COHORT_BUCKET=${PRJ_BUCKET}/${COHORT}\n",
    "\n",
    "gsutil -mq ls ${COHORT_BUCKET}/ubams > ${WRKDIR}/${COHORT}.found.files\n",
    "\n",
    "sed -i s\"/gs:\\/\\/nihnialng-pd-wgs\\/${COHORT}\\/ubams\\///\"g ${WRKDIR}/${COHORT}.found.files\n",
    "sed -i s\"/\\///\"g ${WRKDIR}/${COHORT}.found.files\n",
    "\n",
    "less ${WRKDIR}/${COHORT}.found.files | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for any missing expected bams\n",
    "expected_file = '{}/{}.samples.list'.format(WRKDIR, COHORT)\n",
    "observed_file = '{}/{}.found.files'.format(WRKDIR, COHORT) \n",
    "missing_file = '{}/{}.missing.samples.list'.format(WRKDIR, COHORT)\n",
    "\n",
    "expected = pd.read_csv(expected_file,header=None)\n",
    "observed = pd.read_csv(observed_file,header=None)\n",
    "\n",
    "print(expected.shape)\n",
    "print(observed.shape)\n",
    "\n",
    "print(len(set(expected[0]) - set(observed[0])))\n",
    "\n",
    "missing = expected.loc[~expected[0].isin(observed[0])]\n",
    "print(missing.shape)\n",
    "missing.head()\n",
    "\n",
    "#save the missing list\n",
    "missing.to_csv(missing_file,header=None,index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THIS IS WHERE WE STOPPED AFTER FASTQ TO uBAM coversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean up the temp workspace and logs from the fastq to ubam conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#run these commands at terminal:\\n')\n",
    "print('#WE\\'VE ALREADY RUN THIS SO DO NOT NEED TO CLEANUP AGAIN\\n')\n",
    "\n",
    "print('gsutil -mq rm -r {}/logs/ubams'.format(COHORT_BUCKET))\n",
    "print('gsutil -mq rm -r {}/workspace'.format(COHORT_BUCKET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make sure the json dir is there for the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the jsons directory for the ubam to cram if it doesn't exist\n",
    "#also check to see if the blank jsons are present or you need to retrieve\n",
    "fastq_to_bam_json_dir = '{}/jsons'.format(WRKDIR)\n",
    "\n",
    "if os.path.isdir(fastq_to_bam_json_dir):\n",
    "    os.makedirs(fastq_to_bam_json_dir + '/broadbams', exist_ok=True)    \n",
    "    \n",
    "checkfortemplatefile(fastq_to_bam_json_dir + '/blank.align.label.json')\n",
    "checkfortemplatefile(fastq_to_bam_json_dir + '/PairedEndSingleSampleWf.gatk4.0.options.json')\n",
    "checkfortemplatefile(fastq_to_bam_json_dir + '/template.broadbam.hg38.json')\n",
    "checkfortemplatefile(fastq_to_bam_json_dir + '/cromwell_client.py')\n",
    "checkfortemplatefile(fastq_to_bam_json_dir + '/PairedEndSingleSampleWf.gatk4.0.wdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#run these commands at terminal:\\n')\n",
    "\n",
    "print('gsutil cp gs://nihnialng-pd-wgs/tools/broad/blank.align.label.json {}/jsons/'.format(WRKDIR))\n",
    "print('gsutil cp gs://nihnialng-pd-wgs/tools/broad/PairedEndSingleSampleWf.gatk4.0.options.json \\\n",
    "{}/jsons/'.format(WRKDIR))\n",
    "print('gsutil cp gs://nihnialng-pd-wgs/tools/broad/template.broadbam.hg38.json {}/jsons/'.format(WRKDIR))\n",
    "print('gsutil cp gs://nihnialng-pd-wgs/tools/broad/cromwell_client.py {}/jsons/'.format(WRKDIR))\n",
    "print('gsutil cp gs://nihnialng-pd-wgs/tools/broad/PairedEndSingleSampleWf.gatk4.0.wdl {}/jsons/'.format(WRKDIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create the ubam to cram per sample json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the extra json files for ubams to crams; ie labels and options for alignment wf\n",
    "from datetime import datetime\n",
    "\n",
    "json_label_template = '{}/jsons/blank.align.label.json'.format(WRKDIR)\n",
    "json_options_template = '{}/jsons/PairedEndSingleSampleWf.gatk4.0.options.json'.format(WRKDIR)\n",
    "json_broad_template = '{}/jsons/template.broadbam.hg38.json'.format(WRKDIR)\n",
    "\n",
    "sample_ids = cohort_df['oriID'].unique()\n",
    "for sample_id in sample_ids:\n",
    "    json_labels_outfile_name = '{}/jsons/broadbams/{}.labels.json'.format(WRKDIR,sample_id)\n",
    "    json_options_outfile_name = '{}/jsons/broadbams/{}.options.json'.format(WRKDIR,sample_id)\n",
    "    json_broad_outfile_name = '{}/jsons/broadbams/{}.broadbam.hg38.json'.format(WRKDIR,sample_id)    \n",
    "\n",
    "    #format and write the label json\n",
    "    with open(json_label_template) as json_file:  \n",
    "        label_data = json.load(json_file)\n",
    "        \n",
    "        label_data['cohort'] = COHORT.lower()\n",
    "        label_data['sample'] = sample_id.lower()\n",
    "        label_data['user'] = MYUSER.lower()\n",
    "        \n",
    "        with open(json_labels_outfile_name,'w') as json_outfile:\n",
    "            json.dump(label_data,json_outfile,sort_keys=True,indent=4)   \n",
    "    \n",
    "    #format and write the options json\n",
    "    with open(json_options_template) as json_file:  \n",
    "        options_data = json.load(json_file)\n",
    "        \n",
    "        options_data['final_workflow_outputs_dir'] = '{}/hg38/align-wf/{}'.format(COHORT_BUCKET, sample_id)\n",
    "        options_data['final_workflow_log_dir'] = '{}/logs/{}'.format(COHORT_BUCKET, sample_id)\n",
    "        options_data['final_call_logs_dir'] = '{}/logs/{}'.format(COHORT_BUCKET, sample_id)\n",
    "        \n",
    "        with open(json_options_outfile_name,'w') as json_outfile:\n",
    "            json.dump(options_data,json_outfile,sort_keys=True,indent=4)   \n",
    "\n",
    "    #format and write the broad json\n",
    "    get_ubams_cmd = 'gsutil ls {}/ubams/{}/*.unmapped.bam'.format(COHORT_BUCKET,sample_id)\n",
    "    ubams = !{get_ubams_cmd}\n",
    "\n",
    "    with open(json_broad_template) as json_file:  \n",
    "        broad_data = json.load(json_file)\n",
    "        \n",
    "        broad_data['PairedEndSingleSampleWorkflow.sample_name'] = sample_id\n",
    "        broad_data['PairedEndSingleSampleWorkflow.base_file_name'] = sample_id\n",
    "        broad_data['PairedEndSingleSampleWorkflow.flowcell_unmapped_bams'] = ubams\n",
    "        broad_data['PairedEndSingleSampleWorkflow.final_gvcf_base_name'] = sample_id\n",
    "        \n",
    "        with open(json_broad_outfile_name,'w') as json_outfile:\n",
    "            json.dump(broad_data,json_outfile,sort_keys=False,indent=4)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setup the cromwell server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#setup a cromwell server for running the alignment wdl jobs\\n\\\n",
    "#get Matt\\'s cromwell stuff\\n\\\n",
    "#already pulled for other projects so copy over from dementia_wgs\\n')\n",
    "\n",
    "print('#run these commands at terminal:\\n')\n",
    "print('#I\\'VE ALREADY RUN THIS, SO DO NOT NEED TO CREATE SERVER AGAIN\\n')\n",
    "print('#cp -r ../dementia_wgs/tools/verily-amp-pd-source {}/tools/'.format(WRKDIR))\n",
    "\n",
    "print('\\n#fire up the cromwell instance')\n",
    "print('chmod +x {}/tools/verily-amp-pd-source/setup_cromwell_vm/*.sh'.format(WRKDIR))\n",
    "print('cd {}/tools/verily-amp-pd-source/setup_cromwell_vm/'.format(WRKDIR))\n",
    "print('./create_cromwell_server.sh {}-cromwell {} n1-highmem-8'\\\n",
    "      .format(COHORT,PROJECT_ID))\n",
    "print('./configure.sh {}-cromwell {} nihnialng-pd-wgs/{}'\\\n",
    "      .format(COHORT,PROJECT_ID,COHORT.replace('_','-')))\n",
    "\n",
    "print('\\n#When that is up, ssh to the instance:')\n",
    "print('gcloud --project {} compute ssh {}-cromwell'.format(PROJECT_ID,COHORT.replace('_','-')))\n",
    "\n",
    "print('\\n#And in that SSH session, run:')\n",
    "print('cd /install')\n",
    "print('docker-compose -f /install/workspace/config/docker-compose.yml up')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create the ssh tunnel to the cromwell server so you can submit jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#When cromwell is up, create an SSH tunnel from your workstation:')\n",
    "print('#run these commands at terminal:\\n')\n",
    "print('gcloud --project {} compute ssh {}-cromwell -- -L 8000:localhost:8000'\\\n",
    "      .format(PROJECT_ID,COHORT.replace('_','-')))\n",
    "\n",
    "print('\\n#after this runs you well actually be logged into the cromwell server, ' \\\n",
    "      'so you will need to open another termincal session on your machine to submit your jobs\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define format function the ggp cmd\n",
    "def formatgcpcmd(this_sample,chrt_bucket):\n",
    "    this_cmd = 'echo -n {SAMPLE} OPID=\\n\\\n",
    "python {WRKDIR}/jsons/cromwell_client.py \\\n",
    "--wdl {WRKDIR}/jsons/PairedEndSingleSampleWf.gatk4.0.wdl \\\n",
    "--workflow-inputs {WRKDIR}/jsons/broadbams/{SAMPLE}.broadbam.hg38.json \\\n",
    "--workflow-options {WRKDIR}/jsons/broadbams/{SAMPLE}.options.json \\\n",
    "--workflow-labels {WRKDIR}/jsons/broadbams/{SAMPLE}.labels.json'\n",
    "    return(this_cmd.format(WRKDIR=WRKDIR,COHORT=COHORT,SAMPLE=this_sample))\n",
    "\n",
    "#iterate over samples formatting the cmds\n",
    "cohort_bucket = '{}/{}'.format(PRJ_BUCKET,COHORT)\n",
    "cmds = [formatgcpcmd(sample_id,cohort_bucket) for sample_id in sample_ids]\n",
    "\n",
    "temp_script_file = '{}/{}.run_ubams_to_crams.sh'.format(WRKDIR,COHORT.lower())\n",
    "\n",
    "with open(temp_script_file, 'w') as file_handler:\n",
    "        for this_cmd in cmds:\n",
    "            file_handler.write(\"{}\\n\".format(this_cmd))\n",
    "            \n",
    "print('#run these commands at terminal:\\n')\n",
    "print('chmod +x ' + temp_script_file)\n",
    "print('nohup ' + temp_script_file + ' > {}/{}.run_ubams_to_crams.log &'.format(WRKDIR,COHORT.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see how many GCE (Google Compute Engine) instances are running your jobs\n",
    "PIPELABEL='pairedendsinglesamplewf'\n",
    "\n",
    "print('#full job worker node count')\n",
    "!gcloud compute instances list --project {PROJECT_ID} \\\n",
    "--filter \"labels:({PIPELABEL} {COHORT} {MYUSER})\" | grep RUNNING | wc -l\n",
    "\n",
    "print('#number of all running instances in project')\n",
    "!gcloud compute instances list --project {PROJECT_ID} | grep RUNNING | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### commands for checking statuses using Cromwell REST cmds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#run these commands at terminal:\\n')\n",
    "\n",
    "print('#These actually won\\'t be that helpful because you are all using the same server.\\n')\n",
    "\n",
    "print('#When cromwell is up, create an SSH tunnel from your workstation, if not already connected:\\n')\n",
    "print('gcloud --project {} compute ssh {}-cromwell -- -L 8000:localhost:8000'.\\\n",
    "      format(PROJECT_ID,COHORT.replace('_','-')))\n",
    "\n",
    "print('\\n#if tunnel established can check cromwell status\\n')\n",
    "print('curl -X GET \"http://localhost:8000/api/workflows/v1/query?status=Running\"')\n",
    "print('curl -X GET \"http://localhost:8000/api/workflows/v1/query?status=Submitted\"')\n",
    "print('curl -X GET \"http://localhost:8000/api/workflows/v1/query?status=Failed\"')\n",
    "print('curl -X GET \"http://localhost:8000/api/workflows/v1/query?status=Succeeded\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### commands to check progess by counting expected output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check bam, cram, and gvcf counts\n",
    "print('#These actually won\\'t be that helpful because you are all using the same bucket path.\\n')\n",
    "print('#crams')\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-ConvertToCram/**.cram | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-ConvertToCram/**.crai | wc -l\n",
    "print('#bams')\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-GatherBamFiles/**.bam | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-GatherBamFiles/**.bai | wc -l\n",
    "print('#gvcfs')\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-MergeVCFs/**.g.vcf.gz | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### commands for copying the primary output files to a final dest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#run these commands at terminal:\\n')\n",
    "\n",
    "print('#gvcfs')\n",
    "print('gsutil -mq cp {}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-MergeVCFs/**.g.vcf.gz* \\\n",
    "{}/hg38/gvcfs/'.format(COHORT_BUCKET,COHORT_BUCKET))\n",
    "print('\\n#crams')\n",
    "print('gsutil -mq cp {}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-ConvertToCram/**.cram* \\\n",
    "{}/hg38/crams/'.format(COHORT_BUCKET,COHORT_BUCKET))\n",
    "print('\\n#bams')\n",
    "print('gsutil -mq cp {}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-GatherBamFiles/**.bam \\\n",
    "{}/hg38/bams/'.format(COHORT_BUCKET,COHORT_BUCKET))\n",
    "print('gsutil -mq cp {}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-GatherBamFiles/**.bai \\\n",
    "{}/hg38/bams/'.format(COHORT_BUCKET,COHORT_BUCKET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confirm counts of the primary output files at final dest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check bam, cram, and gvcf counts at final path\n",
    "print('#crams')\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/crams/*.cram | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/crams/*.crai | wc -l\n",
    "print('#bams')\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/bams/*.bam | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/bams/*.bai | wc -l\n",
    "print('#gvcfs')\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/gvcfs/*.g.vcf.gz | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/gvcfs/*.g.vcf.gz.tbi | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check that all expected files are there by looking at sample IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$WRKDIR\" \"$COHORT_BUCKET\" \"$COHORT\"\n",
    "#get a list of files that were successfully created\n",
    "WRKDIR=${1}\n",
    "COHORT_BUCKET=${2}\n",
    "COHORT=${3}\n",
    "\n",
    "gsutil -mq ls ${COHORT_BUCKET}/hg38/crams/*.cram > ${WRKDIR}/${COHORT}.found.files\n",
    "\n",
    "sed -i s\"/gs:\\/\\/nihnialng-pd-wgs\\/${COHORT}\\/hg38\\/crams\\///\"g ${WRKDIR}/${COHORT}.found.files\n",
    "sed -i s\"/\\///\"g ${WRKDIR}/${COHORT}.found.files\n",
    "sed -i s\"/\\.cram//\"g ${WRKDIR}/${COHORT}.found.files\n",
    "\n",
    "less ${WRKDIR}/${COHORT}.found.files | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for any missing expected bams\n",
    "expected_file = '{}/{}.samples.list'.format(WRKDIR, COHORT)\n",
    "observed_file = '{}/{}.found.files'.format(WRKDIR, COHORT) \n",
    "missing_file = '{}/{}.missing.samples.list'.format(WRKDIR, COHORT)\n",
    "\n",
    "expected = pd.read_csv(expected_file,header=None)\n",
    "observed = pd.read_csv(observed_file,header=None)\n",
    "\n",
    "print(expected.shape)\n",
    "print(observed.shape)\n",
    "\n",
    "print(len(set(expected[0]) - set(observed[0])))\n",
    "\n",
    "missing = expected.loc[~expected[0].isin(observed[0])]\n",
    "print(missing.shape)\n",
    "missing.head()\n",
    "\n",
    "#save the missing list\n",
    "missing.to_csv(missing_file,header=None,index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if all the expected primary output files are present then go ahead and copy of the other output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move the other metrics reports that may be of interest to keep\n",
    "#need to move all the other summary metric files over to final output\n",
    "print('#run these commands at terminal:\\n')\n",
    "print('gsutil -mq cp {}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-ValidateCram/**.cram.validation_report ' \\\n",
    "'{}/hg38/align-wf/call-ValidateCram/'.format(COHORT_BUCKET,COHORT_BUCKET))\n",
    "print('\\ngsutil -mq cp ${}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-CheckContamination/**.preBqsr.selfSM ' \\\n",
    "'{}/hg38/align-wf/call-CheckContamination/'.format(COHORT_BUCKET,COHORT_BUCKET))\n",
    "print('\\ngsutil -mq cp {}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/*/call-GatherBqsrReports/**.recal_data.csv ' \\\n",
    "'{}/hg38/align-wf/call-GatherBqsrReports/'.format(COHORT_BUCKET,COHORT_BUCKET))\n",
    "print('\\ngsutil -mq cp {}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/**_metrics {}/hg38/align-wf/metrics/'.\\\n",
    "      format(COHORT_BUCKET,COHORT_BUCKET))\n",
    "print('\\ngsutil -mq cp {}/hg38/align-wf/*/PairedEndSingleSampleWorkflow/**.pdf {}/hg38/align-wf/metrics/'.\\\n",
    "      format(COHORT_BUCKET,COHORT_BUCKET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check file counts at destination for primary and other metrics output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check file counts\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/gvcfs/*.vcf.gz | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/gvcfs/*.vcf.gz.tbi | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/crams/*.cram | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/crams/*.crai | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/bams/*.bam | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/bams/*.bai | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/align-wf/call-ValidateCram/*.cram.validation_report | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/align-wf/call-CheckContamination/*.preBqsr.selfSM | wc -l\n",
    "!gsutil ls {COHORT_BUCKET}/hg38/align-wf/call-GatherBqsrReports/*.recal_data.csv | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check the cram validation reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull down the validation reports and check for erros\n",
    "!mkdir -p {WRKDIR}/temp\n",
    "\n",
    "!gsutil -mq cp {COHORT_BUCKET}/hg38/align-wf/call-ValidateCram/*.cram.validation_report {WRKDIR}/temp/\n",
    "\n",
    "!ls {WRKDIR}/temp/*.validation_report | wc -l\n",
    "!less {WRKDIR}/temp/*.validation_report | grep \"No errors found\" | wc -l\n",
    "\n",
    "#clean up the temp validation reports\n",
    "!rm {WRKDIR}/temp/*.validation_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is where we stopped after alignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### do some additional cleanup, I've already run these deletes\n",
    "\n",
    "including the deletion of the ubams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if successfully, clean up log files and workspace path\n",
    "!echo gsutil -mq rm -r {COHORT_BUCKET}/logs\n",
    "!echo gsutil -mq rm -r {COHORT_BUCKET}/cromwell-execution/PairedEndSingleSampleWorkflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define format function the gcp cmd to delete additional per sample un-needed files\n",
    "def formatgcpcmd(this_sample,chrt_bucket):\n",
    "    this_cmd = 'gsutil -mq rm -r {COHORT_BUCKET}/hg38/align-wf/{SAMPLE}/PairedEndSingleSampleWorkflow'\n",
    "    return(this_cmd.format(COHORT_BUCKET=chrt_bucket,SAMPLE=this_sample))\n",
    "\n",
    "#here reloading sample_ids\n",
    "cohort_file_list = '{}/{}.samples.list'.format(WRKDIR,COHORT)\n",
    "sample_ids = pd.read_csv(cohort_file_list,header=None).to_numpy()\n",
    "sample_ids = sample_ids.reshape(len(sample_ids))\n",
    "\n",
    "#iterate over samples formatting the cmds\n",
    "cmds = [formatgcpcmd(sample_id,COHORT_BUCKET) for sample_id in sample_ids]\n",
    "\n",
    "temp_script_file = '{}/{}.delete_other_files.sh'.format(WRKDIR,COHORT.lower())\n",
    "\n",
    "with open(temp_script_file, 'w') as file_handler:\n",
    "        for this_cmd in cmds:\n",
    "            file_handler.write(\"{}\\n\".format(this_cmd))\n",
    "            \n",
    "print('#run these commands at terminal:\\n')\n",
    "print('#I\\'VE ALREADY RUN THIS SO DO NOT NEED TO DELETE AGAIN\\n')\n",
    "print('chmod +x ' + temp_script_file)\n",
    "print('nohup ' + temp_script_file + ' > {}/{}.delete_other_files.log &'.format(WRKDIR,COHORT.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define format function the ggp cmd to delete ubams\n",
    "def formatgcpcmd(this_sample,chrt_bucket):\n",
    "    this_cmd = 'gsutil -mq rm -r {COHORT_BUCKET}/ubams/{SAMPLE}'\n",
    "    return(this_cmd.format(COHORT_BUCKET=chrt_bucket,SAMPLE=this_sample))\n",
    "\n",
    "#iterate over samples formatting the cmds\n",
    "cmds = [formatgcpcmd(sample_id,COHORT_BUCKET) for sample_id in sample_ids]\n",
    "\n",
    "temp_script_file = '{}/{}.delete_ubams.sh'.format(WRKDIR,COHORT.lower())\n",
    "\n",
    "with open(temp_script_file, 'w') as file_handler:\n",
    "        for this_cmd in cmds:\n",
    "            file_handler.write(\"{}\\n\".format(this_cmd))\n",
    "            \n",
    "print('#run these commands at terminal:\\n')\n",
    "print('#I\\'VE ALREADY RUN THIS SO DO NOT NEED TO DELETE AGAIN\\n')\n",
    "print('chmod +x ' + temp_script_file)\n",
    "print('nohup ' + temp_script_file + ' > {}/{}.delete_ubams.log &'.format(WRKDIR,COHORT.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### archive the fastqs to nearline storage for some periond of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move fastqs to archvie bucket\n",
    "fastq_bucket_path = '{}/{}/fastqs/*.fastq.gz'.format(PRJ_BUCKET,COHORT)\n",
    "archive_bucket_path = 'gs://nihnialng-pd-archive/fastqs/usuhs/{}/'.\\\n",
    "format(COHORTBUILD.replace('.', '_'))\n",
    "\n",
    "print('#run these commands at terminal:\\n')\n",
    "print('#I\\'VE ALREADY RUN THIS SO DO NOT NEED TO MOVE AGAIN\\n')\n",
    "print('nohup gsutil -mq cp {} {} &'.format(fastq_bucket_path, archive_bucket_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that all the files were moved, although should be successful if no error was reported\n",
    "#should match the original counts from early fastq counting cell\n",
    "!gsutil ls {archive_bucket_path} | wc -l\n",
    "\n",
    "#get storage size\n",
    "!gsutil -mq du -hs {archive_bucket_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if everything is ok with archiving of fastqs, delete original staging bucket fastqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the staging bucket fastqs that USUHS uploaded\n",
    "print('#run these commands at terminal:\\n')\n",
    "print('#I\\'VE ALREADY RUN THIS SO DO NOT NEED TO DELETE AGAIN\\n')\n",
    "print('gsutil -mq rm gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.00/*.fastq.gz')\n",
    "print('gsutil -mq rm gs://nihnialng-staging-f745d15a/pA3.hernandez.N148.01/*.fastq.gz')\n",
    "print('gsutil -mq rm {}/fastqs/*.fastq.gz'.format(COHORT_BUCKET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check for any contaminated samples\n",
    "pull down seq contamination check info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull down the verifybamid check contamination files \n",
    "!mkdir -p {WRKDIR}/seqqc\n",
    "\n",
    "!gsutil -mq cp {COHORT_BUCKET}/hg38/align-wf/call-CheckContamination/*.preBqsr.selfSM {WRKDIR}/seqqc/\n",
    "!ls {WRKDIR}/seqqc/*.preBqsr.selfSM | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$WRKDIR\" \"$COHORTBUILD\" \"$COHORT\"\n",
    "#combine the contamination files into single table\n",
    "WRKDIR=${1}\n",
    "COHORTBUILD=${2}\n",
    "COHORT=${3}\n",
    "\n",
    "SAMPLESLISTFILE=\"${WRKDIR}/${COHORT}.samples.list\"\n",
    "METRICSFILE=\"${WRKDIR}/${COHORTBUILD}.contam.metrics.txt\"\n",
    "\n",
    "if [ -s ${METRICSFILE} ]; then\n",
    "rm ${METRICSFILE}\n",
    "fi\n",
    "echo \"id avgdp freemix\" > ${METRICSFILE}\n",
    "\n",
    "ls ${WRKDIR}/seqqc/*.preBqsr.selfSM | xargs -l basename > ${WRKDIR}/selfsm.file.list\n",
    "sed -i s\"/\\.preBqsr\\.selfSM//\"g ${WRKDIR}/selfsm.file.list\n",
    "\n",
    "while read SAMPLELINE\n",
    "do\n",
    "SAMPLE=$(echo ${SAMPLELINE} | awk '{print $1}')\n",
    "awk -v SAMPLE=${SAMPLE} '$1 == SAMPLE {print $1,$6,$7}' \\\n",
    "${WRKDIR}/seqqc/${SAMPLE}.preBqsr.selfSM >> ${METRICSFILE}\n",
    "done < ${WRKDIR}/selfsm.file.list\n",
    "\n",
    "rm ${WRKDIR}/selfsm.file.list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check the contamination reports for problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the contamination and seq coverage rates\n",
    "contam_metrics_file = '{}/{}.contam.metrics.txt'.format(WRKDIR,COHORTBUILD)\n",
    "\n",
    "WARN_FREEMIX = 0.02\n",
    "MAX_FREEMIX = 0.049\n",
    "MIN_DEPTH = 27\n",
    "\n",
    "metrics_df = pd.read_csv(contam_metrics_file,sep='\\s+')\n",
    "print(metrics_df.shape)\n",
    "metrics_df.head()\n",
    "\n",
    "maybe_contamin_df = metrics_df.loc[(metrics_df['freemix'] <= MAX_FREEMIX) & \\\n",
    "                                   (metrics_df['freemix'] > WARN_FREEMIX)]\n",
    "print('number of samples maybe contaminated with freemix > {} and < {} is {}'\\\n",
    "      .format(WARN_FREEMIX,MAX_FREEMIX,maybe_contamin_df.shape[0]))\n",
    "contam_maybe_file = '{}/{}.contamination.possible.samples.txt'.format(WRKDIR,COHORTBUILD)\n",
    "maybe_contamin_df.to_csv(contam_maybe_file,index=False,sep='\\t')\n",
    "\n",
    "contaminated_df = metrics_df.loc[metrics_df['freemix'] > MAX_FREEMIX]\n",
    "print('number of samples with freemix > {} is {}'.format(MAX_FREEMIX,contaminated_df.shape[0]))\n",
    "contam_problems_file = '{}/{}.contaminated.samples.txt'.format(WRKDIR,COHORTBUILD)\n",
    "contaminated_df.to_csv(contam_problems_file,index=False,sep='\\t')\n",
    "\n",
    "low_cov_df = metrics_df.loc[metrics_df['avgdp'] < MIN_DEPTH]\n",
    "print('number of samples with avgdp < {} is {}'.format(MIN_DEPTH,low_cov_df.shape[0]))\n",
    "low_coverage_file = '{}/{}.low_coverage.samples.txt'.format(WRKDIR,COHORTBUILD)\n",
    "low_cov_df.to_csv(low_coverage_file,index=False,sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### good news there aren't any samples with contamination or low coverage\n",
    "\n",
    "if there had been, you would want to just exclude the ones with freemix > 5% and decide on excluding the possibly contaminated or low coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now prep for running the joint genotyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pull down the broad tooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull down the correct recent Broad tooling\n",
    "!mkdir -p {WRKDIR}/tools\n",
    "\n",
    "#don't think wget is easy to install for Mac, maybe use curl instead\n",
    "# !wget https://raw.githubusercontent.com/gatk-workflows/gatk4-germline-snps-indels/master/joint-discovery-gatk4.wdl \\\n",
    "#     -O {WRKDIR}/tools/joint-discovery-gatk4.wdl\n",
    "# !wget https://raw.githubusercontent.com/gatk-workflows/gatk4-germline-snps-indels/master/joint-discovery-gatk4.hg38.wgs.inputs.json \\\n",
    "#     -O {WRKDIR}/tools/joint-discovery-gatk4.hg38.wgs.inputs.json\n",
    "\n",
    "!curl https://raw.githubusercontent.com/gatk-workflows/gatk4-germline-snps-indels/master/joint-discovery-gatk4.wdl \\\n",
    "    -o {WRKDIR}/tools/joint-discovery-gatk4.wdl\n",
    "!curl https://raw.githubusercontent.com/gatk-workflows/gatk4-germline-snps-indels/master/joint-discovery-gatk4.hg38.wgs.inputs.json \\\n",
    "    -o {WRKDIR}/tools/joint-discovery-gatk4.hg38.wgs.inputs.json\n",
    "    \n",
    "!curl https://raw.githubusercontent.com/gatk-workflows/gatk4-germline-snps-indels/master/generic.google-papi.options.json \\\n",
    "    -o {WRKDIR}/tools/generic.google-papi.options.json    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### update joint calling jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update some of the json configs for broad joint calling\n",
    "#generate the extra json files for ubams to crams; ie labels and options for alignment wf\n",
    "\n",
    "generic_options_template = '{}/tools/generic.google-papi.options.json'.format(WRKDIR)\n",
    "wdl_input_template = '{}/tools/joint-discovery-gatk4.hg38.wgs.inputs.json'.format(WRKDIR)\n",
    "\n",
    "json_labels_outfile_name = '{}/jsons/{}.jdgatk4.labels.json'.format(WRKDIR,COHORT)\n",
    "json_options_outfile_name = '{}/jsons/{}.jdgatk4.options.json'.format(WRKDIR,COHORT)\n",
    "json_broad_outfile_name = '{}/jsons/{}.jdgatk4.hg38.wgs.inputs.json'.format(WRKDIR,COHORT)    \n",
    "\n",
    "#format and write the label json\n",
    "label_data = {}\n",
    "label_data['workflow'] = 'jdgatk4'\n",
    "label_data['cohort'] = COHORT.lower()\n",
    "label_data['user'] = MYUSER.lower()\n",
    "\n",
    "with open(json_labels_outfile_name,'w') as json_outfile:\n",
    "    json.dump(label_data,json_outfile,sort_keys=True,indent=4)   \n",
    "\n",
    "#format and write the options json\n",
    "with open(generic_options_template) as json_file:  \n",
    "    options_data = json.load(json_file)\n",
    "    \n",
    "    options_data['read_from_cache'] = True\n",
    "    options_data['write_to_cache'] = True\n",
    "    options_data['workflow_failure_mode'] = 'ContinueWhilePossible'\n",
    "    options_data['system.input-read-limits.lines'] = 640000    \n",
    "    options_data['final_workflow_outputs_dir'] = '{}/hg38/joint_calling'.format(COHORT_BUCKET)\n",
    "    options_data['final_workflow_log_dir'] = '{}/logs/joint_calling'.format(COHORT_BUCKET)\n",
    "    options_data['final_call_logs_dir'] = '{}/logs/joint_calling'.format(COHORT_BUCKET)\n",
    "\n",
    "    with open(json_options_outfile_name,'w') as json_outfile:\n",
    "        json.dump(options_data,json_outfile,sort_keys=True,indent=4)   \n",
    "\n",
    "#format and write the broad json\n",
    "with open(wdl_input_template) as json_file:  \n",
    "    broad_data = json.load(json_file)\n",
    "    \n",
    "    sample_gvcfs_path = '{}/{}.sample.gvcfs.map.txt'.format(COHORT_BUCKET,COHORTBUILD)\n",
    "    broad_data['JointGenotyping.callset_name'] = COHORTBUILD\n",
    "    broad_data['JointGenotyping.sample_name_map'] = sample_gvcfs_path\n",
    "\n",
    "    with open(json_broad_outfile_name,'w') as json_outfile:\n",
    "        json.dump(broad_data,json_outfile,sort_keys=False,indent=4)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$WRKDIR\" \"$COHORT_BUCKET\" \"$COHORT\"\n",
    "#get a list of files that were successfully created\n",
    "WRKDIR=${1}\n",
    "COHORT_BUCKET=${2}\n",
    "COHORT=${3}\n",
    "\n",
    "gsutil -mq ls ${COHORT_BUCKET}/hg38/crams/*.cram > ${WRKDIR}/${COHORT}.found.files\n",
    "\n",
    "sed -i s\"/gs:\\/\\/nihnialng-pd-wgs\\/${COHORT}\\/hg38\\/crams\\///\"g ${WRKDIR}/${COHORT}.found.files\n",
    "sed -i s\"/\\///\"g ${WRKDIR}/${COHORT}.found.files\n",
    "sed -i s\"/\\.cram//\"g ${WRKDIR}/${COHORT}.found.files\n",
    "\n",
    "less ${WRKDIR}/${COHORT}.found.files | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to create the gvcfs sample map file used as json argument above JointGenotyping.sample_name_map\n",
    "#instead of just looping expected ids do so from found list in previous cell, allow to move/remove fails\n",
    "\n",
    "found_file_list = '{}/{}.found.files'.format(WRKDIR,COHORT)\n",
    "found_ids = pd.read_csv(found_file_list,header=None).to_numpy()\n",
    "# found_ids = sample_ids.reshape(len(found_ids))\n",
    "found_ids = found_ids.reshape(len(found_ids))\n",
    "\n",
    "gvcfs_map_file = '{}/{}.sample.gvcfs.map.txt'.format(WRKDIR,COHORTBUILD)\n",
    "\n",
    "with open(gvcfs_map_file, 'w') as file_handler:\n",
    "    for sample_id in found_ids:\n",
    "        file_handler.write(\"{}\\t{}/hg38/gvcfs/{}.g.vcf.gz\\n\".\\\n",
    "                           format(sample_id,COHORT_BUCKET,sample_id))\n",
    "\n",
    "#now copy the map file up to cloud bucket\n",
    "!gsutil -mq cp {gvcfs_map_file} {COHORT_BUCKET}/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now run the joint calling job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#When cromwell is up, create an SSH tunnel from your workstation:')\n",
    "print('#run these commands at terminal:\\n')\n",
    "print('gcloud --project {} compute ssh {}-cromwell -- -L 8000:localhost:8000'\\\n",
    "      .format(PROJECT_ID,COHORT.replace('_','-')))\n",
    "\n",
    "print('\\n#after this runs you well actually be logged into the cromwell server, ' \\\n",
    "      'so you will need to open another termincal session on your machine to submit your jobs\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now run the joint calling job\n",
    "\n",
    "run_cmd = 'python {wrk_dir}/jsons/cromwell_client.py \\\n",
    "--wdl {wrk_dir}/tools/oint-discovery-gatk4.wdl \\\n",
    "--workflow-inputs {wrk_dir}/jsons/{this_cohort}.jdgatk4.hg38.wgs.inputs.json \\\n",
    "--workflow-options {wrk_dir}/jsons/{this_cohort}.jdgatk4.options.json \\\n",
    "--workflow-labels {wrk_dir}/jsons/{this_cohort}.jdgatk4.labels.json'.\\\n",
    "format(wrk_dir=WRKDIR, this_cohort=COHORT)\n",
    "\n",
    "print('#run these commands at terminal:\\n')\n",
    "\n",
    "print('{} > {}/{}.jdgatk4.jobid'.format(run_cmd, WRKDIR, COHORTBUILD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This where we stopped after joint calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see how many GCE (Google Compute Engine) instances are running for joint calling\n",
    "print('#full job worker node count')\n",
    "!gcloud compute instances list --project {PROJECT_ID} \\\n",
    "--filter \"labels:(jdgatk4 {COHORT} {MYUSER})\" | grep RUNNING | wc -l\n",
    "\n",
    "print('#number of all running instances in project')\n",
    "!gcloud compute instances list --project {PROJECT_ID} | grep RUNNING | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### commands for checking statuses using Cromwell REST cmds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#run these commands at terminal:\\n')\n",
    "\n",
    "print('#These actually won\\'t be that helpful because you are all using the same server.\\n')\n",
    "\n",
    "print('#When cromwell is up, create an SSH tunnel from your workstation, if not already connected:\\n')\n",
    "print('gcloud --project {} compute ssh {}-cromwell -- -L 8000:localhost:8000'.\\\n",
    "      format(PROJECT_ID,COHORT.replace('_','-')))\n",
    "\n",
    "print('\\n#if tunnel established can check cromwell status\\n')\n",
    "print('curl -X GET \"http://localhost:8000/api/workflows/v1/query?status=Running\"')\n",
    "print('curl -X GET \"http://localhost:8000/api/workflows/v1/query?status=Submitted\"')\n",
    "print('curl -X GET \"http://localhost:8000/api/workflows/v1/query?status=Failed\"')\n",
    "print('curl -X GET \"http://localhost:8000/api/workflows/v1/query?status=Succeeded\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check to make sure expected file(s) are there in this case since < 1K samples will be single vcf\n",
    "\n",
    "Note that below here would be different for larger cohorts, with file per chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see if result file is present\n",
    "!gsutil ls -lh {COHORT_BUCKET}/hg38/joint_calling/JointGenotyping/*/call-FinalGatherVcf/{COHORTBUILD}.vcf.gz*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### move file to a final destination path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#job was run twice to two sets of outputs, grab one and then do some cleanup\n",
    "print('#run these commands at terminal:\\n')\n",
    "print('#I\\'VE ALREADY RUN THIS SO DO NOT NEED TO RUN AGAIN\\n')\n",
    "in_path = f'{COHORT_BUCKET}/hg38/joint_calling/JointGenotyping/83cca38d-7e42-4ac1-bb7c-278c1343a6d7'\n",
    "out_path = f'{COHORT_BUCKET}/hg38/JointGenotyping/'\n",
    "print(f'gsutil -mq cp {in_path }/call-FinalGatherVcf/{COHORTBUILD}.vcf.gz* {out_path}')\n",
    "print(f'gsutil -mq cp {in_path }/call-CollectMetricsOnFullVcf/* {out_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now do some clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete logging, cromwell execution path, and detritus from output path\n",
    "print('#run these commands at terminal:\\n')\n",
    "print('#I\\'VE ALREADY RUN THIS SO DO NOT NEED TO RUN AGAIN\\n')\n",
    "\n",
    "print(f'gsutil -mq rm {in_path }/call-FinalGatherVcf/{COHORTBUILD}.vcf.gz*')\n",
    "print(f'gsutil -mq rm {in_path }/call-CollectMetricsOnFullVcf/*')\n",
    "print(f'gsutil -mq rm {in_path }/call-DynamicallyCombineIntervals/*')\n",
    "print(f'gsutil -mq rm -r {COHORT_BUCKET}/hg38/joint_calling/JointGenotyping/2a88a45a-f6dd-4d61-9bfb-d70d07f04fb7')\n",
    "\n",
    "print(f'gsutil -mq rm -r {COHORT_BUCKET}/logs')\n",
    "print(f'gsutil -mq rm -r {COHORT_BUCKET}/cromwell-execution/JointGenotyping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now delete the cromwell server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if done with the cromwell server delete it\n",
    "print('#run these commands at terminal:\\n')\n",
    "print('#I\\'VE ALREADY RUN THIS SO DO NOT NEED TO RUN AGAIN\\n')\n",
    "!echo gcloud compute instances delete {COHORT.replace('_','-')}-cromwell \\\n",
    "--project {PROJECT_ID} --zone us-central1-f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subset vcfs to just PASSED variants\n",
    "\n",
    "#### VQSR doesn't really work for the non-PAR sexomes and while you probably won't be doing analsys on sexome variation you will need at lest chrX to perform sex check QC\n",
    "\n",
    "so submit another vcf subset but only for chrX without PASS filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#run these commands at terminal:\\n')\n",
    "#prep and run the VQSR subset job\n",
    "print(f'cut -f 1 {WRKDIR}/{COHORTBUILD}.sample.gvcfs.map.txt > {WRKDIR}/{COHORTBUILD}.samples.list\\n')\n",
    "print(f'gsutil -mq cp {WRKDIR}/{COHORTBUILD}.samples.list {COHORT_BUCKET}/\\n')\n",
    "\n",
    "pipe_yaml = 'gs://nihnialng-pd-wgs/tools/yamls/SubjectSubsetVCF.yaml'\n",
    "\n",
    "filter_vqsr_cmd = f'gcloud alpha genomics pipelines run \\\n",
    "--project {PROJECT_ID} \\\n",
    "--pipeline-file {pipe_yaml} \\\n",
    "--logging {COHORT_BUCKET}/logs/jointcall/filtervcf/ \\\n",
    "--inputs INPUTVCF={COHORT_BUCKET}/hg38/JointGenotyping/{COHORTBUILD}.vcf.gz \\\n",
    "--inputs INPUTVCFINDEX={COHORT_BUCKET}/hg38/JointGenotyping/{COHORTBUILD}.vcf.gz.tbi \\\n",
    "--inputs SUBJECTLIST={COHORT_BUCKET}/{COHORTBUILD}.samples.list \\\n",
    "--inputs EXCLUDECARROT=\\\" \\\" \\\n",
    "--inputs FILTERFLAGS=\\\"-f PASS --force-samples\\\" \\\n",
    "--inputs MINAC=\\\"1\\\" \\\n",
    "--outputs OUTVCF={COHORT_BUCKET}/hg38/genotypes/{COHORTBUILD}.vcf.gz \\\n",
    "--outputs OUTVCFINDEX={COHORT_BUCKET}/hg38/genotypes/{COHORTBUILD}.vcf.gz.tbi \\\n",
    "--labels=pipe=filtervcfs,cohort={COHORT.lower()},user={MYUSER}\\n'\n",
    "\n",
    "print(filter_vqsr_cmd)\n",
    "\n",
    "chrx_vcf_cmd = f'gcloud alpha genomics pipelines run \\\n",
    "--project {PROJECT_ID} \\\n",
    "--pipeline-file {pipe_yaml} \\\n",
    "--logging {COHORT_BUCKET}/logs/jointcall/filtervcf/ \\\n",
    "--inputs INPUTVCF={COHORT_BUCKET}/hg38/JointGenotyping/{COHORTBUILD}.vcf.gz \\\n",
    "--inputs INPUTVCFINDEX={COHORT_BUCKET}/hg38/JointGenotyping/{COHORTBUILD}.vcf.gz.tbi \\\n",
    "--inputs SUBJECTLIST={COHORT_BUCKET}/{COHORTBUILD}.samples.list \\\n",
    "--inputs EXCLUDECARROT=\\\" \\\" \\\n",
    "--inputs FILTERFLAGS=\\\"--regions chrX --force-samples\\\" \\\n",
    "--inputs MINAC=\\\"1\\\" \\\n",
    "--outputs OUTVCF={COHORT_BUCKET}/hg38/genotypes/{COHORTBUILD}.chrX.vcf.gz \\\n",
    "--outputs OUTVCFINDEX={COHORT_BUCKET}/hg38/genotypes/{COHORTBUILD}.chrX.vcf.gz.tbi \\\n",
    "--labels=pipe=filtervcfs,cohort={COHORT.lower()},chromosome=chrx,user={MYUSER}\\n'\n",
    "\n",
    "print(chrx_vcf_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check status of filter on VQSR pass job using OPID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the above google genomics pipeline (ggp) commands should return on OPIDs\n",
    "#grab to previous operation IDs to use here as opids list\n",
    "opids = ['EKXH4N7RLRiQuKK7mLHWsX4gpIDW36QGKg9wcm9kdWN0aW9uUXVldWU', \\\n",
    "'EKXH4N7RLRiQuKK7mLHWsX4gpIDW36QGKg9wcm9kdWN0aW9uUXVldWU']\n",
    "\n",
    "for opid in opids:\n",
    "    !gcloud alpha genomics operations describe {opid} \\\n",
    "    --format='yaml(done, error, metadata.events)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split multi-allelics, name variants, and thin out vcf verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split multi-allelics, name variants, and thin out vcf verbosity\n",
    "pipe_yaml = 'gs://nihnialng-pd-wgs/tools/yamls/SplitAndNameVCF.yaml'\n",
    "\n",
    "print('#run these commands at terminal:\\n')\n",
    "split_name_cmd = f'gcloud alpha genomics pipelines run \\\n",
    "--project {PROJECT_ID} \\\n",
    "--pipeline-file {pipe_yaml} \\\n",
    "--logging {COHORT_BUCKET}/logs/jointcall/splitname/ \\\n",
    "--inputs INPUTVCF={COHORT_BUCKET}/hg38/genotypes/{COHORTBUILD}.vcf.gz \\\n",
    "--inputs INPUTVCFINDEX={COHORT_BUCKET}/hg38/genotypes/{COHORTBUILD}.vcf.gz.tbi \\\n",
    "--inputs REFFASTA=gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta \\\n",
    "--inputs REFFASTAINDEX=gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai \\\n",
    "--outputs OUTVCF={COHORT_BUCKET}/hg38/genotypes/{COHORTBUILD}.gtonly.vcf.gz \\\n",
    "--outputs OUTVCFINDEX={COHORT_BUCKET}/hg38/genotypes/{COHORTBUILD}.gtonly.vcf.gz.tbi \\\n",
    "--labels=pipe=splitnamevcf,cohort={COHORT.lower()},user={MYUSER}\\n'\n",
    "\n",
    "print(split_name_cmd)\n",
    "\n",
    "split_name_cmd = f'gcloud alpha genomics pipelines run \\\n",
    "--project {PROJECT_ID} \\\n",
    "--pipeline-file {pipe_yaml} \\\n",
    "--logging {COHORT_BUCKET}/logs/jointcall/splitname/ \\\n",
    "--inputs INPUTVCF={COHORT_BUCKET}/hg38/genotypes/{COHORTBUILD}.chrX.vcf.gz \\\n",
    "--inputs INPUTVCFINDEX={COHORT_BUCKET}/hg38/genotypes/{COHORTBUILD}.chrX.vcf.gz.tbi \\\n",
    "--inputs REFFASTA=gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta \\\n",
    "--inputs REFFASTAINDEX=gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai \\\n",
    "--outputs OUTVCF={COHORT_BUCKET}/hg38/genotypes/{COHORTBUILD}.chrX.gtonly.vcf.gz \\\n",
    "--outputs OUTVCFINDEX={COHORT_BUCKET}/hg38/genotypes/{COHORTBUILD}.chrX.gtonly.vcf.gz.tbi \\\n",
    "--labels=pipe=splitnamevcf,cohort={COHORT.lower()},user={MYUSER}\\n'\n",
    "\n",
    "print(split_name_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check status on the split multi-allelics, re-IDs, and gtonly job using OPID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the above google genomics pipeline (ggp) commands should return on OPIDs\n",
    "#grab to previous operation IDs to use here as opids list\n",
    "opids = ['EIaD2unRLRil9uOBusqGsH8gpIDW36QGKg9wcm9kdWN0aW9uUXVldWU', \\\n",
    "'ENfu2unRLRjw7-eD1sqevNEBIKSA1t-kBioPcHJvZHVjdGlvblF1ZXVl']\n",
    "\n",
    "for opid in opids:\n",
    "    !gcloud alpha genomics operations describe {opid} \\\n",
    "    --format='yaml(done, error, metadata.events)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirm output file was successfull created\n",
    "!gsutil -mq ls -lh {COHORT_BUCKET}/hg38/genotypes/{COHORTBUILD}.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#run these commands at terminal:\\n')\n",
    "#clean up log files\n",
    "print(f'gsutil -mq rm -r {COHORT_BUCKET}/logs/jointcall/filtervcf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run QC on samples\n",
    "\n",
    "you can all run this separately in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pull down the genotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {WRKDIR}/genotypes\n",
    "\n",
    "!gsutil -mq cp {COHORT_BUCKET}/hg38/genotypes/{COHORTBUILD}.*gtonly.vcf.gz* {WRKDIR}/genotypes/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prior to converting from vcf to plink helps to go ahead and include dx and dx\n",
    "\n",
    "can format using plink2 sample file psam\n",
    "#FID IID sex dx\n",
    "\n",
    "hold over from plink1 sex; male=1, female=2, unknown/other=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cohort psam\n",
    "sample_psam = f'{WRKDIR}/genotypes/{COHORT}.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert vcfs to plink2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert from vcf to plink2\n",
    "input_vcf = f'{WRKDIR}/genotypes/{COHORTBUILD}.gtonly.vcf.gz'\n",
    "out_file_set = f'{WRKDIR}/genotypes/{COHORTBUILD}'\n",
    "\n",
    "!/Users/mooreank/Downloads/plink2 --vcf {input_vcf} --double-id \\\n",
    "--update-sex /Users/mooreank/Desktop/WGS/genotypes/ninds_eopd.txt col-num=3 \\\n",
    "--pheno /Users/mooreank/Desktop/WGS/genotypes/ninds_eopd.txt --pheno-col-nums 4 \\\n",
    "--silent --allow-extra-chr --make-pgen --out {out_file_set}\n",
    "\n",
    "input_vcf = f'{WRKDIR}/genotypes/{COHORTBUILD}.chrX.gtonly.vcf.gz'\n",
    "out_file_set = f'{WRKDIR}/genotypes/{COHORTBUILD}.chrX'\n",
    "\n",
    "!/Users/mooreank/Downloads/plink2 --vcf {input_vcf} --double-id \\\n",
    "--update-sex /Users/mooreank/Desktop/WGS/genotypes/ninds_eopd.txt col-num=3 \\\n",
    "--pheno /Users/mooreank/Desktop/WGS/genotypes/ninds_eopd.txt --pheno-col-nums 4 \\\n",
    "--silent --allow-extra-chr --make-pgen --out {out_file_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check creation and logging\n",
    "!ls {WRKDIR}/genotypes/{COHORTBUILD}.*\n",
    "\n",
    "!tail {WRKDIR}/genotypes/{COHORTBUILD}.*log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trim variants to QC set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trim variant to QC set\n",
    "input_file_set = f'{WRKDIR}/genotypes/{COHORTBUILD}'\n",
    "out_file_set = f'{WRKDIR}/qc/{COHORTBUILD}.geno05maf05hwe000001'\n",
    "\n",
    "!mkdir -p {WRKDIR}/qc\n",
    "\n",
    "!/Users/mooreank/Downloads/plink2 --pfile {input_file_set} \\\n",
    "--geno 0.05 --maf 0.05 --hwe 0.000001 \\\n",
    "--silent --make-bed --out {out_file_set}\n",
    "\n",
    "input_file_set = f'{WRKDIR}/genotypes/{COHORTBUILD}.chrX'\n",
    "out_file_set = f'{WRKDIR}/qc/{COHORTBUILD}.chrX.geno05maf05'\n",
    "\n",
    "!/Users/mooreank/Downloads/plink2 --pfile {input_file_set} \\\n",
    "--geno 0.05 --maf 0.05 \\\n",
    "--silent --make-bed --out {out_file_set}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run sexcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$COHORTBUILD\" \"$WRKDIR\"\n",
    "#check gender\n",
    "COHORTBUILD=${1}\n",
    "WRKDIR=${2}\"/qc\"\n",
    "\n",
    "#hg38 non-PAR\n",
    "awk '$4 > 2800000 && $4 < 155700000 {print $2}' ${WRKDIR}/${COHORTBUILD}.chrX.geno05maf05.bim \\\n",
    "    > ${WRKDIR}/${COHORTBUILD}.chrX.list\n",
    "\n",
    "plink --bfile ${WRKDIR}/${COHORTBUILD}.chrX.geno05maf05 --extract ${WRKDIR}/${COHORTBUILD}.chrX.list \\\n",
    "--check-sex 0.25 0.75 --silent --out ${WRKDIR}/${COHORTBUILD}.sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check sex test results\n",
    "sexcheck_df = pd.read_csv(f'{WRKDIR}/qc/{COHORTBUILD}.sex.sexcheck', sep='\\s+')\n",
    "print(f'sexcheck shape is {sexcheck_df.shape}')\n",
    "\n",
    "fail_sexcheck_df = sexcheck_df.loc[sexcheck_df['STATUS'] == 'PROBLEM']\n",
    "print(f'number of samples failing sexcheck {fail_sexcheck_df.shape[0]}')\n",
    "print(fail_sexcheck_df)\n",
    "\n",
    "fail_sexcheck_df = sexcheck_df.loc[(sexcheck_df['STATUS'] == 'PROBLEM') & \\\n",
    "                                   (sexcheck_df['PEDSEX'] != 0)]\n",
    "print(f'number of samples failing sexcheck excluding missing info {fail_sexcheck_df.shape}')\n",
    "print(fail_sexcheck_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check missingness\n",
    "!plink2 --bfile {WRKDIR}/qc/{COHORTBUILD}.geno05maf05hwe000001 --missing --autosome \\\n",
    "--silent --out {WRKDIR}/qc/{COHORTBUILD}.missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check missingness results\n",
    "misstest_df = pd.read_csv(f'{WRKDIR}/qc/{COHORTBUILD}.missing.lmiss', sep='\\s+')\n",
    "\n",
    "#find failed\n",
    "misstest_failed_df = misstest_df.loc[misstest_df['F_MISS'] > 0.05]\n",
    "\n",
    "print(f'number samples failing missingness test {misstest_failed_df.shape[0]}')\n",
    "\n",
    "print(misstest_failed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check het rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check het rates\n",
    "!plink --bfile {WRKDIR}/qc/{COHORTBUILD}.geno05maf05hwe000001 --het --autosome \\\n",
    "--silent --out {WRKDIR}/qc/{COHORTBUILD}.het"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check het rate results\n",
    "\n",
    "###2 failes\n",
    "\n",
    "hets_df = pd.read_csv(f'{WRKDIR}/qc/{COHORTBUILD}.het.het', sep='\\s+')\n",
    "\n",
    "#find failed\n",
    "hets_failed_df = hets_df.loc[(hets_df['F'] > 0.15) | (hets_df['F'] < -0.15)]\n",
    "\n",
    "print(f'number samples failing heterzygosity check {hets_failed_df.shape[0]}')\n",
    "\n",
    "print(hets_failed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check for relatedness, including duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# king_related_cmd = f'nohup king --related -b {WRKDIR}/qc/{COHORTBUILD}.geno05maf05hwe000001.bed \\\n",
    "# --cpus $(nproc) --degree 2 --prefix {WRKDIR}/qc/{COHORTBUILD}.king.related \\\n",
    "# > {WRKDIR}/qc/{COHORTBUILD}.king.related.log &'\n",
    "\n",
    "# king_related_cmd = f'nohup /Users/mooreank/Downloads/king --related -b {WRKDIR}/qc/{COHORTBUILD}.geno05maf05hwe000001.bed \\\n",
    "#  --degree 2 --prefix {WRKDIR}/qc/{COHORTBUILD}.king.related \\\n",
    "# > {WRKDIR}/qc/{COHORTBUILD}.king.related.log &'\n",
    "\n",
    "\n",
    "king_related_cmd = f'king --related -b /labshare/anni/from_local/{COHORTBUILD}.geno05maf05hwe000001.bed \\\n",
    " --degree 2 --prefix /labshare/anni/from_local/{COHORTBUILD}.king.related \\\n",
    "> /labshare/anni/from_local/{COHORTBUILD}.king.related.log &'\n",
    "\n",
    "print('#run these commands at terminal:\\n')\n",
    "print(king_related_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the king log\n",
    "!tail -n 16 {WRKDIR}/qc/{COHORTBUILD}.king.related.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for the duplicates see which ones also failed sexcheck, ie clues how to correctly resolve duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the sex problems\n",
    "sexcheck = pd.read_csv(f'{WRKDIR}/qc/{COHORTBUILD}.sex.sexcheck',sep='\\s+')\n",
    "sexmismatch = sexcheck.loc[(sexcheck['PEDSEX'] != 0) & (sexcheck['STATUS'] == 'PROBLEM')]\n",
    "\n",
    "#get the duplicates\n",
    "rels_df = pd.read_csv(f'{WRKDIR}/qc/{COHORTBUILD}.king.related.kin0',sep='\\s+')\n",
    "print(f'number of related pairs {rels_df.shape[0]}')\n",
    "print(rels_df['InfType'].value_counts())\n",
    "\n",
    "#subset just the dups\n",
    "dups_df = rels_df.loc[rels_df['InfType'] == 'DUP/MZ']\n",
    "\n",
    "print(f'\\njust the DUP/MZ {dups_df.shape[0]}')\n",
    "print(dups_df['InfType'].value_counts())\n",
    "\n",
    "print('\\nnumber of samples in that failed sexcheck and also a duplicate')\n",
    "print(len(set(sexmismatch['IID']) & (set(rels_df['ID1']) | set(rels_df['ID2']))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check the sample ancestries making use of 1KG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull down the 1KG genotypes to look at pop-structure\n",
    "!gsutil -mq cp gs://nihnialng-pd-wgs/tools/onekg.good.geno05maf05hwe000001.* {WRKDIR}/qc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the pull downs\n",
    "!ls -lhtr {WRKDIR}/qc/onekg.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$WRKDIR\"\n",
    "#for our WGS since I renamed all variants by position and allele need to do the same for onekg data\n",
    "WRKDIR=${1}\"/qc\"\n",
    "\n",
    "mv ${WRKDIR}/onekg.good.geno05maf05hwe000001.bim ${WRKDIR}/onekg.good.geno05maf05hwe000001.bim.ori\n",
    "awk '{ print $1\"\\tchr\"$1\":\"$4\":\"$5\":\"$6\"\\t\"$3\"\\t\"$4\"\\t\"$5\"\\t\"$6}' \\\n",
    "${WRKDIR}/onekg.good.geno05maf05hwe000001.bim.ori > ${WRKDIR}/onekg.good.geno05maf05hwe000001.bim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$WRKDIR\" \"$COHORTBUILD\"\n",
    "#unfortunelty because of the plink2/plink1 ref vs minor freq variant files, \\\n",
    "#so rename the variants to match the flipped alleles\n",
    "WRKDIR=${1}\"/qc\"\n",
    "COHORTBUILD=${2}\n",
    "\n",
    "mv ${WRKDIR}/${COHORTBUILD}.geno05maf05hwe000001.bim ${WRKDIR}/${COHORTBUILD}.geno05maf05hwe000001.bim.ori\n",
    "awk '{ print $1\"\\tchr\"$1\":\"$4\":\"$5\":\"$6\"\\t\"$3\"\\t\"$4\"\\t\"$5\"\\t\"$6}' \\\n",
    "${WRKDIR}/${COHORTBUILD}.geno05maf05hwe000001.bim.ori > ${WRKDIR}/${COHORTBUILD}.geno05maf05hwe000001.bim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$COHORTBUILD\" \"$WRKDIR\"\n",
    "#merge cohort and 1KG data with plink\n",
    "COHORTBUILD=${1}\n",
    "WRKDIR=${2}\"/qc\"\n",
    "\n",
    "plink --bfile ${WRKDIR}/${COHORTBUILD}.geno05maf05hwe000001 \\\n",
    "--bmerge ${WRKDIR}/onekg.good.geno05maf05hwe000001 \\\n",
    "    --geno 0.05 --maf 0.05 --hwe 0.000001 --silent --allow-no-sex \\\n",
    "    --keep-allele-order --make-bed --out ${WRKDIR}/${COHORTBUILD}.onekg\n",
    "\n",
    "if [ -s ${WRKDIR}/${COHORTBUILD}.onekg-merge.missnp ]; then\n",
    "plink2 -bfile ${WRKDIR}/${COHORTBUILD}.geno05maf05hwe000001 --allow-no-sex \\\n",
    "    --exclude ${WRKDIR}/${COHORTBUILD}.onekg-merge.missnp --silent \\\n",
    "    --keep-allele-order --make-bed --out ${WRKDIR}/${COHORTBUILD}.geno05maf05hwe000001.temp\n",
    "\n",
    "plink2 -bfile ${WRKDIR}/onekg.good.geno05maf05hwe000001 --allow-no-sex \\\n",
    "    --exclude ${WRKDIR}/${COHORTBUILD}.onekg-merge.missnp --silent \\\n",
    "    --keep-allele-order --make-bed --out ${WRKDIR}/onekg.good.geno05maf05hwe000001.temp\n",
    "\n",
    "plink --bfile ${WRKDIR}/${COHORTBUILD}.geno05maf05hwe000001.temp \\\n",
    "    --bmerge ${WRKDIR}/onekg.good.geno05maf05hwe000001.temp \\\n",
    "    --geno 0.05 --maf 0.05 --hwe 0.000001 --silent --allow-no-sex \\\n",
    "    --keep-allele-order --make-bed --out ${WRKDIR}/${COHORTBUILD}.onekg\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$COHORTBUILD\" \"$WRKDIR\"\n",
    "#merge cohort and 1KG data with plink\n",
    "COHORTBUILD=${1}\n",
    "WRKDIR=${2}\"/qc\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the merged cohort and 1KG data with plink was created\n",
    "!ls -lh {WRKDIR}/qc/{COHORTBUILD}.onekg*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pull down the 1KG population labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to pull down the 1KG population labels\n",
    "!gsutil -mq cp gs://nihnialng-pd-wgs/tools/onekg.panel {WRKDIR}/qc/\n",
    "    \n",
    "#check file\n",
    "!head {WRKDIR}/qc/onekg.panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### king is to slow for population background check once we get into the thousands of samples\n",
    "\n",
    "for now switch back to LD prunning, plink genome ibs/ibd and mds methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$COHORTBUILD\" \"$WRKDIR\"\n",
    "#merge cohort and 1KG data with plink\n",
    "COHORTBUILD=${1}\n",
    "WRKDIR=${2}\"/qc\"\n",
    "\n",
    "echo -e \"#run these commands at terminal:\\n\"\n",
    "\n",
    "#find pruned variants\n",
    "echo plink2 --bfile ${WRKDIR}/${COHORTBUILD}.onekg \\\n",
    "--indep-pairwise 50 5 0.3 --out ${WRKDIR}/${COHORTBUILD}.onekg.ldprune \n",
    "\n",
    "#subset to pruned variants\n",
    "echo plink2 --bfile ${WRKDIR}/${COHORTBUILD}.onekg \\\n",
    "--extract ${WRKDIR}/${COHORTBUILD}.onekg.ldprune.prune.in \\\n",
    "--make-bed --out ${WRKDIR}/${COHORTBUILD}.onekg.pruned\n",
    "\n",
    "#generate ibs/ibd info\n",
    "echo plink --bfile ${WRKDIR}/${COHORTBUILD}.onekg.pruned \\\n",
    "--genome --out ${WRKDIR}/${COHORTBUILD}.onekg.ibs_ibd\n",
    "\n",
    "#do mds clusters from ibs/ibd data\n",
    "echo \"nohup plink --bfile ${WRKDIR}/${COHORTBUILD}.onekg.pruned \\\n",
    "--read-genome ${WRKDIR}/${COHORTBUILD}.onekg.ibs_ibd.genome \\\n",
    "--cluster --mds-plot 20 --out ${WRKDIR}/${COHORTBUILD}.onekg.ibs_ibd.mds &\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_dir =  f'{WRKDIR}/qc'\n",
    "mdsfile = f'{qc_dir}/{COHORTBUILD}.onekg.ibs_ibd.mds.mds'\n",
    "popinfofile =  f'{qc_dir}/onekg.panel'\n",
    "output_pca_plot = f'{qc_dir}/{COHORTBUILD}.pop_structure.pca.png'\n",
    "outfile = f'{qc_dir}/{COHORTBUILD}.pop.outliers.txt'\n",
    "\n",
    "cohort_label = COHORT\n",
    "\n",
    "expected_ancestry = 'EUR'\n",
    "\n",
    "mdsdf = pd.read_csv(mdsfile,sep='\\s+')\n",
    "infodf = pd.read_csv(popinfofile,sep=\"\\t\")\n",
    "\n",
    "df = mdsdf.merge(infodf,how='left',left_on='IID',right_on='sample')\n",
    "\n",
    "df[['super_pop']] = df[['super_pop']].fillna(cohort_label)\n",
    "df[['pop']] = df[['pop']].fillna(cohort_label)\n",
    "\n",
    "ancestry_cohort = df.loc[df['super_pop'] == expected_ancestry]\n",
    "coi = df.loc[df['super_pop'] == cohort_label]\n",
    "\n",
    "mds1_mean = np.mean(ancestry_cohort['C1'])\n",
    "mds1_std = np.std(ancestry_cohort['C1'])\n",
    "mds2_mean = np.mean(ancestry_cohort['C2'])\n",
    "mds2_std = np.std(ancestry_cohort['C2'])\n",
    "\n",
    "maxSD = 6\n",
    "plusMax_1 = mds1_mean + maxSD * mds1_std\n",
    "negMax_1 = mds1_mean - maxSD * mds1_std\n",
    "plusMax_2 = mds2_mean + maxSD * mds2_std\n",
    "negMax_2 = mds2_mean - maxSD * mds2_std\n",
    "\n",
    "df['outlier'] = ((df['super_pop'] == cohort_label) & \\\n",
    "    ((df['C1'] > plusMax_1) | (df['C1'] < negMax_1) | \\\n",
    "     (df['C2'] > plusMax_2) | (df['C2'] < negMax_2)))\n",
    "\n",
    "outliers = df.loc[df['outlier'] == True] \n",
    "outliers.to_csv(outfile,sep=\"\\t\",index=False,header=False)\n",
    "\n",
    "df.loc[df['IID'].isin(outliers['IID']),['super_pop','pop']] = 'outlier'\n",
    "\n",
    "# df = df.sort_values('super_pop',ascending=False)\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "cohort_euro_df = df[df['super_pop'].isin([expected_ancestry,cohort_label,'outlier'])]\n",
    "# cohort_euro_df = cohort_euro_df.sort_values('pop',ascending=False)\n",
    "# cohort_euro_df = cohort_euro_df.sample(frac=1)\n",
    "\n",
    "sns.set()\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "sns.scatterplot(x='C1',y='C2',hue='super_pop',style='super_pop',data=df)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=1, borderaxespad=0,prop={'size': 8})\n",
    "plt.subplot(1,2,2)\n",
    "sns.scatterplot(x='C1',y='C2',hue='pop',style='pop',data=cohort_euro_df)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0,prop={'size': 8})\n",
    "#plt.legend(loc='upper right', prop={'size': 6})\n",
    "plt.savefig(output_pca_plot,format='png',dpi=600,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_euro_df['pop'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdsdf.set_index(mdsdf['IID'],inplace=True)\n",
    "mdsdf.drop(columns=['FID','IID','SOL'],inplace=True)\n",
    "mdsdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run UMAP on the expression data\n",
    "from umap import UMAP\n",
    "\n",
    "umap_results = UMAP(random_state=42).fit_transform(mdsdf)\n",
    "df_umap = pd.DataFrame(umap_results,columns=['x-umap','y-umap'], \\\n",
    "                                   index=mdsdf.index).round(3)\n",
    "print(f'The dimensions of the umap df of the population ancestry data are {df_umap.shape}')\n",
    "df_umap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_umap = df_umap.merge(infodf,how='left',left_index=True,right_on='sample')\n",
    "\n",
    "df_umap[['super_pop']] = df_umap[['super_pop']].fillna(cohort_label)\n",
    "df_umap[['pop']] = df_umap[['pop']].fillna(cohort_label)\n",
    "\n",
    "df_umap.loc[df_umap['sample'].isin(outliers['IID']),['super_pop','pop']] = 'outlier'\n",
    "\n",
    "print(df_umap['super_pop'].value_counts())\n",
    "\n",
    "cohort_euro_df = df_umap[df_umap['super_pop'].isin([expected_ancestry,cohort_label,'outlier'])]\n",
    "print(cohort_euro_df['pop'].value_counts())\n",
    "\n",
    "# cohort_euro_df = cohort_euro_df.sort_values('sample',ascending=False)\n",
    "cohort_euro_df = cohort_euro_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_umap_plot = f'{qc_dir}/{COHORTBUILD}.pop_structure.umap.png'\n",
    "\n",
    "sns.set()\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "sns.scatterplot(x='x-umap',y='y-umap',hue='super_pop',style='super_pop',data=df_umap)\n",
    "plt.xlabel('x-UMAP')\n",
    "plt.ylabel('y-UMAP')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=1, borderaxespad=0,prop={'size': 8})\n",
    "plt.subplot(1,2,2)\n",
    "sns.scatterplot(x='x-umap',y='y-umap',hue='pop',data=cohort_euro_df)\n",
    "plt.xlabel('x-UMAP')\n",
    "plt.ylabel('y-UMAP')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0,prop={'size': 8})\n",
    "plt.savefig(output_umap_plot,format='png',dpi=600,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accumulate definite excludes, ie sample QC fails\n",
    "missingness, sexcheck, het-rate, and contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sex check mismatches\n",
    "sexcheck_file = f'{WRKDIR}/qc/{COHORTBUILD}.sex.sexcheck'\n",
    "sexcheck_df = pd.read_csv(sexcheck_file,sep='\\s+')\n",
    "sexcheck_problems_df = sexcheck_df[(sexcheck_df['STATUS'] == 'PROBLEM') & (sexcheck_df['PEDSEX'] != 0)]\n",
    "print('sex mismatches')\n",
    "print(f'{sexcheck_problems_df.shape[0]} failed sexcheck')\n",
    "\n",
    "#check missingness \n",
    "smiss_file = f'{WRKDIR}/qc/{COHORTBUILD}.missing.lmiss'\n",
    "smiss_df = pd.read_csv(smiss_file,sep='\\s+')\n",
    "smiss_problems_df = smiss_df[smiss_df['F_MISS'] > 0.05]\n",
    "print(f'{smiss_problems_df.shape[0]} failed missingness check')\n",
    "\n",
    "#het rate problems\n",
    "hetcheck_file = f'{WRKDIR}/qc/{COHORTBUILD}.het.het'\n",
    "hetcheck_df = pd.read_csv(hetcheck_file,sep='\\s+')\n",
    "hetcheck_problems_df = hetcheck_df[(hetcheck_df['F'] > 0.15) | (hetcheck_df['F'] < -0.15)]\n",
    "print(f'{hetcheck_problems_df.shape[0]} failed het rate check')\n",
    "\n",
    "#sample contamination problems\n",
    "# contam_df = pd.DataFrame(data=None,columns=['id'])\n",
    "contam_file = f'{WRKDIR}/{COHORTBUILD}.contaminated.samples.txt'\n",
    "contam_df = pd.read_csv(contam_file,sep='\\s+')\n",
    "print(f'{contam_df.shape[0]} failed contamination check')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def_exclude_set = set(sexcheck_problems_df['IID']) | set(smiss_problems_df['IID']) | \\\n",
    "# set(hetcheck_problems_df['IID']) | set(contam_df['id']) \n",
    "\n",
    "one=set(sexcheck_problems_df['IID'])\n",
    "two=set(smiss_problems_df['SNP'])\n",
    "three=set(hetcheck_problems_df['IID'])\n",
    "four=set(contam_df['id'])\n",
    "\n",
    "def_exclude_set = one | two | three | four\n",
    "\n",
    "\n",
    "\n",
    "print(f'{len(def_exclude_set)} samples should be excluded to exclude')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
